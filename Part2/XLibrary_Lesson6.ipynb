{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection of useful class and function definitions from Fastai part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import PIL # only used for image\n",
    "import mimetypes\n",
    "from functools import partial\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from typing import Iterable, Collection\n",
    "\n",
    "import math\n",
    "import re\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import spacy # NLP tokenizer\n",
    "import html # NLP preprocessing, clean up html stuff\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# turns things into lists\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "\n",
    "def setify(o):\n",
    "    return o if isinstance(o, set) else set(listify(o))\n",
    "\n",
    "def uniqueify(x, sort=False): \n",
    "    # turn a list into a list with unique elements\n",
    "    # and keep the ORDER of the elements unchanged\n",
    "    res = list(OrderedDict.fromkeys(x).keys()) # orderDict will only keep unique keys\n",
    "    if sort: res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda x: list(x.iterdir()) # monkey patch to allow Path to have ls method\n",
    "\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
    "\n",
    "def children(m): return list(m.children())\n",
    "\n",
    "def get_batch(dl, learn):\n",
    "    learn.xb,learn.yb = next(iter(dl))\n",
    "    learn.do_begin_fit(0)\n",
    "    learn('begin_batch')\n",
    "    learn('after_fit')\n",
    "    return learn.xb,learn.yb\n",
    "\n",
    "def find_modules(m, cond):\n",
    "    if cond(m): return [m]\n",
    "    return sum([find_modules(o,cond) for o in m.children()], [])\n",
    "\n",
    "def is_lin_layer(l):\n",
    "    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear)\n",
    "    return isinstance(l, lin_layers)\n",
    "\n",
    "# apply several functions to data x and replace x in place\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key):\n",
    "        x = f(x, **kwargs)\n",
    "    return x\n",
    "\n",
    "# get all supported image file extensions\n",
    "def get_image_extensions():\n",
    "    return set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))\n",
    "\n",
    "def format_time(t):\n",
    "    t = int(t)\n",
    "    h,m,s = t//3600, (t//60)%60, t%60\n",
    "    if h!= 0: return f'{h}:{m:02d}:{s:02d}'\n",
    "    else:     return f'{m:02d}:{s:02d}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# this function gets all the fnames within a given directory,\n",
    "# excluding hidden files that starts with '.'\n",
    "# include all files when an extension is not given\n",
    "# or if extensions is given, if the given ones is within our target extension list\n",
    "# NOTE: the extensions need to start with a '.', e.g. '.jpg'\n",
    "def _get_files(directory, fnames, extensions=None):\n",
    "    p = Path(directory)\n",
    "    res = [p/f for f in fnames if not f.startswith('.')\n",
    "          and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "               \n",
    "# this function walk through all directory and return all files\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    # include = None == include all\n",
    "    path = Path(path) # pathlib is easier to use\n",
    "    extensions = setify(extensions) # remove duplicates\n",
    "    extensions = {e.lower() for e in extensions} # lowercased\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i, (p,d,f) in enumerate(os.walk(path)): # returns (dirpath, folder names, filenames)\n",
    "            # each iteration this goes a layer deeper into sub-directories\n",
    "            if include is not None and i==0:\n",
    "                d[:] = [o for o in d if o in include] # include some folders, not all\n",
    "            else:\n",
    "                d[:] = [o for o in d if not o.startswith('.')] # include all folders except the hidden ones\n",
    "            res += _get_files(p,f,extensions)\n",
    "        return res\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        return _get_files(path, f, extensions)\n",
    "\n",
    "# usage example:\n",
    "# all_fnames = get_files(file_path, image_extensions, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a container class that can contain a list of objects. It will behave a bit like a numpy array in the sense that we can index into it via:\n",
    "- a single index\n",
    "- a slice (like 1:5)\n",
    "- a list of indices\n",
    "- a mask of indices (`[True,False,False,True,...]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ListContainer():\n",
    "    def __init__(self, items): self.items = listify(items)\n",
    "    def __getitem__(self, idx):\n",
    "        try: return self.items[idx]\n",
    "        except TypeError:\n",
    "            if isinstance(idx[0],bool):\n",
    "                assert len(idx)==len(self) # bool mask\n",
    "                return [o for m,o in zip(idx,self.items) if m]\n",
    "            return [self.items[i] for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ItemList(ListContainer):\n",
    "    def __init__(self, items, path='.', tfms=None):\n",
    "        super().__init__(items) # the list of objects can be subscripted like a numpy array\n",
    "        self.path, self.tfms = Path(path), tfms\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{super().__repr__()}\\nPath: {self.path}' # print out list objects and the related path\n",
    "    \n",
    "    def new(self, items, cls=None): # create a new cls-class object \n",
    "        if cls is None:\n",
    "            cls = self.__class__\n",
    "        return cls(items, self.path, tfms=self.tfms)\n",
    "    \n",
    "    def get(self, i): # this method needs to be overloaded by subclasses to explain how to access an element\n",
    "        return i\n",
    "    \n",
    "    def _get(self, i): # apply transforms to item i\n",
    "        return compose(self.get(i), self.tfms) \n",
    "    \n",
    "    def __getitem__(self, idx): # apply transform and then return\n",
    "        res = super().__getitem__(idx)\n",
    "        if isinstance(res, list):\n",
    "            return [self._get(o) for o in res]\n",
    "        return self._get(res)\n",
    "\n",
    "# for image application only\n",
    "class ImageList(ItemList): # inherent ItemList methods\n",
    "    @classmethod # this is to be called directly by class name\n",
    "    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):\n",
    "        if extensions is None:\n",
    "            extensions = get_image_extensions()\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, fname): # overload the parent class get to show how to access elements\n",
    "        return PIL.Image.open(fname)\n",
    "\n",
    "# NLP specifc\n",
    "def read_file(fn):\n",
    "    with open(fn,'r', encoding='utf8') as f:\n",
    "        return f.read() # this is reading all the contents\n",
    "    \n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "        return cls(get_files(path, extensions,recurse=recurse,include=include), path, **kwargs)\n",
    "    # get_files is a standalone function that return all file paths in all folders\n",
    "    # the second 'path' is for the parent ItemList initialization\n",
    "    # Note the entire get_files(path, extensions,recurse=recurse,include=include)\n",
    "    # is items input in the ItemList class initialization\n",
    "    \n",
    "    def get(self,i):\n",
    "        # overload parent get method --> show how to access individual data\n",
    "        if isinstance(i, Path):\n",
    "            return read_file(i)\n",
    "        return i\n",
    "# Example:\n",
    "# item_list = TextList.from_files(file_path, include=['train','test','unsup']) # concat together\n",
    "# __getitem__ work flow:\n",
    "# 1. root class list_container returns the item (a file path in here) corresponding to the index\n",
    "# 2. the item(s) then go through ItemList private _get method and take any transformation provided\n",
    "# 3. public get method is called and use the TextList get method to read the item (i.e. file path)\n",
    "# txt = item_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Transform(): _order=0\n",
    "\n",
    "class MakeRGB(Transform):\n",
    "    def __call__(self, item): return item.convert('RGB')\n",
    "\n",
    "def make_rgb(item): return item.convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# split by grandparent folder\n",
    "def grandparent_splitter(fn, valid_name='valid', train_name='train'):\n",
    "    # return boolean masks for a single item split by folder name at the grandparent level of path\n",
    "    gp = fn.parent.parent.name # define grandparent\n",
    "    return True if gp==valid_name else False if gp==train_name else None\n",
    "\n",
    "# usage example\n",
    "#splitter = partial(grandparent_splitter, valid_name='val')\n",
    "\n",
    "# roughly p_valid ratio of data are chosen as \n",
    "def random_splitter(fn, p_valid): return random.random() < p_valid\n",
    "\n",
    "# Example:\n",
    "# sd = SplitData.split_by_func(item_list, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_by_func(items, f):\n",
    "    mask = [f(o) for o in items]\n",
    "    # `None` values will be filtered out\n",
    "    f = [o for o,m in zip(items, mask) if m==False] # the False set by given criterion f\n",
    "    t = [o for o,m in zip(items, mask) if m==True] # the True set\n",
    "    return f, t \n",
    "\n",
    "# container to automate splitting and transforming data\n",
    "class SplitData(): # unlabeled raw data holder\n",
    "    def __init__(self, train, valid): # to be init by class method\n",
    "        self.train = train # this is a class object with same class as given\n",
    "        self.valid = valid\n",
    "    \n",
    "    def __getattr__(self, k): #\n",
    "        return getattr(self.train, k) # to be handled by the class that train object belongs to\n",
    "    \n",
    "    #This is needed if we want to pickle SplitData and be able to load it back without recursion errors\n",
    "    def __setstate__(self,data):\n",
    "        self.__dict__.update(data)\n",
    "    \n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nTrain: {self.train}\\nValid: {self.valid}\\n'\n",
    "    \n",
    "    @classmethod\n",
    "    def split_by_func(cls, il, f):\n",
    "        lists = map(il.new, split_by_func(il.items, f)) # il needs to have a new method\n",
    "        return cls(*lists) # fed into __init__\n",
    "# usage example:\n",
    "#sd = SplitData.split_by_func(il, splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Processor():\n",
    "    def process(self,items): return items\n",
    "\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None: # build vocab and reverse dict\n",
    "            self.vocab = uniqueify(items) # it's a list\n",
    "            self.otoi = {v:k for k,v in enumerate(self.vocab)} # reverse dict\n",
    "        return [self.proc1(o) for o in items] # return indices of vocab\n",
    "    \n",
    "    def proc1(self,item): \n",
    "        return self.otoi[item]\n",
    "\n",
    "    def deprocess(self,idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(i) for i in idxs]\n",
    "    \n",
    "    def deproc1(self,idx): return self.vocab[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# label by parent folder name\n",
    "def parent_labeler(fn): return fn.parent.name\n",
    "\n",
    "def _label_by_func(ds, f, cls=ItemList):\n",
    "    return cls([f(o) for o in ds.items], path = ds.path) # create ItemList object\n",
    "\n",
    "class LabeledData(): # container for x, y type of data, i.e. labeled data\n",
    "    def process(self, il, proc): # use ItemList method\n",
    "        # apply process proc to data and return ItemList objects\n",
    "        return il.new(compose(il.items,proc))\n",
    "    \n",
    "    def __init__(self,x,y,proc_x=None,proc_y=None):\n",
    "        self.x = self.process(x,proc_x)\n",
    "        self.y = self.process(y,proc_y)\n",
    "        self.proc_x = proc_x\n",
    "        self.proc_y = proc_y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    def __len__(self): return len(self.x)\n",
    "    \n",
    "    def x_obj(self,idx): return self.obj(self.x, idx, self.proc_x)\n",
    "    def y_obj(self,idx): return self.obj(self.y, idx, self.proc_y)\n",
    "    \n",
    "    def obj(self,items,idx,procs):\n",
    "        isint = isinstance(idx,int) or (isinstance(idx,torch.LongTensor) and not idx.ndim) # int or tensor index\n",
    "        item = items[idx]\n",
    "        for proc in reversed(listify(procs)):\n",
    "            item = proc.deproc1(item) if isint else proc.deprocess(item) # if item is a list\n",
    "        return item\n",
    "    \n",
    "    @classmethod # this is the important method to use in practice\n",
    "    def label_by_func(cls, il, f, proc_x=None, proc_y=None):\n",
    "        return cls(il, _label_by_func(il, f), proc_x=proc_x,proc_y=proc_y)  \n",
    "\n",
    "# use the above class, take SplitData object and return SplitData object\n",
    "def label_by_func(sd, f, proc_x=None, proc_y=None):\n",
    "    train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    # the vocab has been built after labeling the train set\n",
    "    # so the validation set will just use the exisiting labels\n",
    "    valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    return SplitData(train,valid)\n",
    "\n",
    "# usage example:\n",
    "# ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databunch related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
    "\n",
    "class DataBunch(): # to hold all the data together, in dataloader form\n",
    "    def __init__(self, train_dl, valid_dl, c_in=None, c_out=None):\n",
    "        self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out\n",
    "\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "\n",
    "# Group the above two things together \n",
    "def databunchify(sd, bs, c_in=None, c_out=None, **kwargs):\n",
    "    dls = get_dls(sd.train, sd.valid, bs, **kwargs)\n",
    "    return DataBunch(*dls, c_in=c_in, c_out=c_out)\n",
    "\n",
    "# monkey patch for SplitData class\n",
    "SplitData.to_databunch = databunchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback():\n",
    "    _order=0\n",
    "    def set_runner(self, run): self.run=run\n",
    "    def __getattr__(self, k): return getattr(self.run, k)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        f = getattr(self, cb_name, None)\n",
    "        if f and f(): return True\n",
    "        return False\n",
    "\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs=0.\n",
    "        self.run.n_iter=0\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter   += 1\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs=self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train=True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train=False\n",
    "\n",
    "class CancelTrainException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
    "\n",
    "    def reset(self):\n",
    "        self.tot_loss,self.count = 0.,0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "\n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self.count: return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
    "\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "    \n",
    "    def begin_fit(self):\n",
    "        met_names = ['loss']+[m.__name__ for m in self.train_stats.metrics]\n",
    "        names = ['epoch'] + [f'train_{n}' for n in met_names] + [\n",
    "            f'valid_{n}' for n in met_names] + ['time']\n",
    "        self.logger(names)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        #We use the logger function of the `Learner` here, it can be customized to write in a file or in a progress bar\n",
    "        stats = [str(self.epoch)]\n",
    "        for o in [self.train_stats, self.valid_stats]:\n",
    "            stats += [f'{v:.6f}' for v in o.avg_stats]\n",
    "        stats += [format_time(time.time() - self.start_time)]\n",
    "        self.logger(stats) # the printing format can be easily revised\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CudaCallback(Callback):\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()\n",
    "\n",
    "class BatchTransformXCallback(Callback):\n",
    "    _order=2\n",
    "    def __init__(self, tfm): self.tfm = tfm\n",
    "    def begin_batch(self): self.run.xb = self.tfm(self.xb)\n",
    "        \n",
    "class DebugCallback(Callback):\n",
    "    _order = 999\n",
    "    def __init__(self, cb_name, f=None): self.cb_name,self.f = cb_name,f\n",
    "    def __call__(self, cb_name):\n",
    "        if cb_name==self.cb_name:\n",
    "            if self.f: self.f(self.run)\n",
    "            else:      set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# parameter (learning rate, momentum, etc) scheduler and finder related\n",
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "@annealer\n",
    "def sched_no(start, end, pos):  return start\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
    "\n",
    "#This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))\n",
    "\n",
    "def combine_scheds(pcts, scheds):\n",
    "    assert sum(pcts) == 1.\n",
    "    pcts = tensor([0] + listify(pcts))\n",
    "    assert torch.all(pcts >= 0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    return _inner\n",
    "\n",
    "def cos_1cycle_anneal(start, high, end):\n",
    "    return [sched_cos(start, high), sched_cos(high, end)]\n",
    "\n",
    "def create_phases(phases):\n",
    "    phases = listify(phases)\n",
    "    return phases + [1-sum(phases)]\n",
    "\n",
    "# these new versions essentially just change the pytorch optim.param_groups to our own hyper dict\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self): plt.plot(self.lrs)\n",
    "    def plot_loss(self): plt.plot(self.losses)\n",
    "        \n",
    "    def plot(self, skip_last=0):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(self.lrs[:n], losses[:n])\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_funcs):\n",
    "        self.pname,self.sched_funcs = pname,listify(sched_funcs)\n",
    "\n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        fs = self.sched_funcs\n",
    "        if len(fs)==1: fs = fs*len(self.opt.param_groups)\n",
    "        pos = self.n_epochs/self.epochs\n",
    "        for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)\n",
    "            \n",
    "class LR_Find(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
    "        self.best_loss = 1e9\n",
    "        \n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
    "        for pg in self.opt.hypers: pg['lr'] = lr\n",
    "            \n",
    "    def after_step(self):\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss: self.best_loss = self.loss\n",
    "            \n",
    "            \n",
    "# final 1-cycle scheduler\n",
    "def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):\n",
    "    phases = create_phases(pct_start)\n",
    "    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "                 for lr in lrs]\n",
    "    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))\n",
    "    return [ParamScheduler('lr', sched_lr),\n",
    "            ParamScheduler('mom', sched_mom)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional callbacks, do NOT export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this requires fastai library\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time\n",
    "\n",
    "class ProgressCallback(Callback):\n",
    "    _order=-1\n",
    "    def begin_fit(self):\n",
    "        self.mbar = master_bar(range(self.epochs))\n",
    "        self.mbar.on_iter_begin()\n",
    "        self.run.logger = partial(self.mbar.write, table=True)\n",
    "        \n",
    "    def after_fit(self): self.mbar.on_iter_end()\n",
    "    def after_batch(self): self.pb.update(self.iter)\n",
    "    def begin_epoch   (self): self.set_pb()\n",
    "    def begin_validate(self): self.set_pb()\n",
    "        \n",
    "    def set_pb(self):\n",
    "        self.pb = progress_bar(self.dl, parent=self.mbar, auto_update=False)\n",
    "        self.mbar.update(self.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# utility functions for optimizer classes\n",
    "def get_defaults(d):\n",
    "    return getattr(d,'_defaults', {})\n",
    "\n",
    "# update the default hyperparameters using the built-in \n",
    "# _default dict value from the steppers if no default \n",
    "# has been provided by the user\n",
    "def maybe_update(steppers, defaults, get_stepper_defaults):\n",
    "    for step in steppers:\n",
    "        for k, v in get_stepper_defaults(step).items():\n",
    "            if k not in defaults:\n",
    "                defaults[k] = v\n",
    "\n",
    "def debias(mom,damp,step):\n",
    "    return damp * (1 - mom**step) / (1-mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# take default values of the steppers when none is provided\n",
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        self.steppers = listify(steppers)\n",
    "        maybe_update(self.steppers, defaults, get_defaults) \n",
    "        # might be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # ensure params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for pg in self.param_groups] # same default for each param_group\n",
    "\n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params(): \n",
    "            compose(p, self.steppers, **hyper)\n",
    "\n",
    "class StatefulOptimizer(Optimizer):\n",
    "    def __init__(self, params, steppers, stats=None, **defaults):\n",
    "        self.stats = listify(stats)\n",
    "        maybe_update(self.stats, defaults, get_defaults)\n",
    "        # only difference with Optimizer is that it keeps track of stats \n",
    "        super().__init__(params, steppers, **defaults)\n",
    "        self.state = {} # can be many different kinds of states\n",
    "        \n",
    "    def step(self):\n",
    "        for p, hyper in self.grad_params(): # for each param with a gradient\n",
    "            if p not in self.state: # if it's not being recorded\n",
    "                self.state[p] = {} # initialize the dictionary\n",
    "                # update default values only if not exist already\n",
    "                maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))\n",
    "            state = self.state[p] # default states of the parameter \n",
    "            for stat in self.stats: # multiple stats can be recorded\n",
    "                state = stat.update(p, state, **hyper) # handle each stat using their own class\n",
    "            compose(p,self.steppers,**state, **hyper) # does self.steppers(p, **state, **hyper)\n",
    "            self.state[p] = state # save current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# states/quantities to remember over training steps\n",
    "class Stat(): # base Stat class\n",
    "    _defaults = {}\n",
    "    def init_state(self, p):\n",
    "        raise NotImplementedError\n",
    "    def update(self, p, state, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Adam needs momentum dampening so we added it to AvgGrad\n",
    "class AverageGrad(Stat):\n",
    "    _defaults = dict(mom=0.9)\n",
    "    def __init__(self, dampening:bool=False):\n",
    "        self.dampening = dampening\n",
    "    def init_state(self, p):\n",
    "        return {'grad_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self, p, state, mom, **kwargs):\n",
    "        # mom_damp = 1 - mom # mom_damp = 1 is undamped\n",
    "        state['mom_damp'] = 1.-mom if self.dampening else 1.\n",
    "        # grad_avg = mom*grad_avg - mom_damp*p.grad\n",
    "        state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)\n",
    "        return state\n",
    "\n",
    "# record moving avg of gradient squared\n",
    "class AverageSqrGrad(Stat):\n",
    "    _defaults = dict(sqr_mom=0.99)\n",
    "    def __init__(self, dampening:bool=True):\n",
    "        self.dampening = dampening\n",
    "    def init_state(self,p):\n",
    "        return {'sqr_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self,p,state,sqr_mom,**kwargs):\n",
    "        state['sqr_damp'] = 1 - sqr_mom if self.dampening else 1.\n",
    "        # sqr_avg = sqr_mom*sqr_avg + sqr_damp*p.grad^2\n",
    "        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)\n",
    "        return state\n",
    "\n",
    "# count number of steps done during training for debiasing\n",
    "class StepCount(Stat):\n",
    "    def init_state(self, p):\n",
    "        return {'step':0}\n",
    "    def update(self,p,state,**kwargs):\n",
    "        state['step'] +=1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# steppers\n",
    "\n",
    "# basic sgd\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(-lr, p.grad.data) # this is p -= lr*p.grad\n",
    "    return p\n",
    "# Example:\n",
    "# opt_func = partial(Optimizer, steppers=[sgd_step])\n",
    "\n",
    "# weight decay\n",
    "def weight_decay(p, lr, wd, **kwargs):\n",
    "    p.data.mul_(1-lr*wd)\n",
    "    return p\n",
    "weight_decay._defaults = dict(wd=0.) # set a hyperparameter _default\n",
    "\n",
    "# L2 regularization\n",
    "def l2_reg(p, lr, wd, **kwargs):\n",
    "    p.grad.data.add_(wd, p.data)\n",
    "    return p\n",
    "l2_reg._defaults = dict(wd=0.)\n",
    "# Example: SGD with weight decay\n",
    "sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])\n",
    "\n",
    "\n",
    "# A momentum stepper that can use the grad_avg state\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    p.data.add_(-lr, grad_avg) # p -= lr*grad_avg\n",
    "    return p\n",
    "# Example: momentum\n",
    "sgd_mom_opt = partial(StatefulOptimizer, steppers = [momentum_step,weight_decay],\n",
    "                     stats=AverageGrad(), wd=0.01)\n",
    "\n",
    "\n",
    "# Adam step --> !!! look at the original formula!\n",
    "def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    debias1 = debias(mom, mom_damp, step)\n",
    "    debias2 = debias(sqr_mom, sqr_damp, step)\n",
    "    # p -= lr/(debias1) * grad_avg/(sqrt(sqr_avg/debias2) + eps)\n",
    "    p.data.addcdiv_(-lr/debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)\n",
    "    return p\n",
    "adam_step._defaults=dict(eps=1e-5)\n",
    "\n",
    "def adam_opt(xtra_step=None, **kwargs):\n",
    "    return partial(StatefulOptimizer, steppers=[adam_step, weight_decay]+listify(xtra_step),\n",
    "                  stats=[AverageGrad(dampening=True), AverageSqrGrad(),StepCount()], **kwargs)\n",
    "\n",
    "# LAMB stepper\n",
    "def lamb_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, wd, **kwargs):\n",
    "    debias1 = debias(mom, mom_damp, step)\n",
    "    debias2 = debias(sqr_mom, sqr_damp, step)\n",
    "    r1 = p.data.pow(2).mean().sqrt()\n",
    "    step = (grad_avg/debias1)/((sqr_avg/debias2).sqrt()+eps) + wd*p.data # weight decay built in here\n",
    "    r2 = step.pow(2).mean().sqrt()\n",
    "    # where does this min come from?\n",
    "    p.data.add_(-lr*min(r1/r2, 10), step)\n",
    "    return p\n",
    "lamb_step._defaults = dict(eps=1e-6, wd=0.)\n",
    "\n",
    "def lamb_opt(xtra_step=None, **kwargs):\n",
    "    return partial(StatefulOptimizer, steppers=[lamb_step]+listify(xtra_step),\n",
    "                  stats=[AverageGrad(dampening=True), AverageSqrGrad(),StepCount()], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def param_getter(m): return m.parameters()\n",
    "\n",
    "class Learner():\n",
    "    # remember in callback logic, False means normal and continue\n",
    "    ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',\n",
    "        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',\n",
    "        'begin_epoch', 'begin_validate', 'after_epoch',\n",
    "        'after_cancel_train', 'after_fit'}\n",
    "    \n",
    "    def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=1e-2,\n",
    "                splitter=param_getter, cbs=None, cb_funcs=None):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.loss_func = loss_func\n",
    "        self.opt_func = opt_func # used to initialize self.opt when needed\n",
    "        self.lr = lr\n",
    "        self.splitter = splitter\n",
    "        \n",
    "        self.in_train = False\n",
    "        self.logger = print\n",
    "        self.opt = None # note this is the real optimizer\n",
    "        \n",
    "        self.cbs = []\n",
    "        self.add_cb(TrainEvalCallback())\n",
    "        self.add_cbs(cbs) # add all callback objects first\n",
    "        self.add_cbs(cbf() for cbf in listify(cb_funcs))\n",
    "    \n",
    "    def add_cb(self, cb):\n",
    "        cb.set_runner(self) # register this Learner in the callbacks\n",
    "        setattr(self, cb.name, cb) # add this callback as a attribute in Learner\n",
    "        self.cbs.append(cb) # update cbs list\n",
    "    \n",
    "    def add_cbs(self,cbs): # add list of callbacks\n",
    "        for cb in listify(cbs):\n",
    "            self.add_cb(cb)\n",
    "    \n",
    "    def remove_cbs(self,cbs):\n",
    "        for cb in listify(cbs):\n",
    "            self.cbs.remove(cb)\n",
    "    \n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        assert cb_name in self.ALL_CBS\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): \n",
    "            res = cb(cb_name) and res\n",
    "        return res\n",
    "    \n",
    "    def one_batch(self, i, xb, yb):\n",
    "        try:\n",
    "            self.iter = i\n",
    "            self.xb, self.yb = xb, yb\n",
    "            self('begin_batch')  \n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            \n",
    "            if not self.in_train: return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException:\n",
    "            self('after_cancel_batch')\n",
    "        finally:\n",
    "            self('after_batch')\n",
    "    \n",
    "    def all_batches(self):\n",
    "        self.iters = len(self.dl) # number of batches\n",
    "        try:\n",
    "            for i, (xb, yb) in enumerate(self.dl):\n",
    "                self.one_batch(i, xb, yb)\n",
    "        except CancelEpochException:\n",
    "            self('after_cancel_epoch')\n",
    "    \n",
    "    def do_begin_fit(self, epochs):\n",
    "        self.epochs, self.loss = epochs, torch.tensor(0.)\n",
    "        self('begin_fit')\n",
    "    \n",
    "    def do_begin_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        self.dl = self.data.train_dl # detail depends on databunch\n",
    "        return self('begin_epoch')\n",
    "    \n",
    "    def fit(self, epochs, cbs=None, reset_opt=False):\n",
    "        # pass extra callbacks to fit() and have them removed when done\n",
    "        self.add_cbs(cbs)\n",
    "        # create optimizer on fit(), optionally replacing existing\n",
    "        if reset_opt or not self.opt:\n",
    "            self.opt = self.opt_func(self.splitter(self.model), lr = self.lr)\n",
    "        \n",
    "        try:\n",
    "            self.do_begin_fit(epochs)\n",
    "            for epoch in range(epochs):\n",
    "                self.do_begin_epoch(epoch)\n",
    "                if not self('begin_epoch'): self.all_batches()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    self.dl = self.data.valid_dl # change data to validation set\n",
    "                    if not self('begin_validate'): self.all_batches()\n",
    "                self('after_epoch')\n",
    "                \n",
    "        except CancelTrainException:\n",
    "            self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.remove_cbs(cbs)\n",
    "\n",
    "def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,\n",
    "                cb_funcs=None, opt_func=sgd_opt, **kwargs):\n",
    "    model = get_cnn_model(data, nfs, layer, **kwargs)\n",
    "    init_cnn(model)\n",
    "    return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Hook():\n",
    "    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[])\n",
    "    means,stds = hook.stats\n",
    "    if mod.training:\n",
    "        means.append(outp.data.mean())\n",
    "        stds .append(outp.data.std())\n",
    "\n",
    "class Hooks(ListContainer):\n",
    "    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "\n",
    "    def remove(self):\n",
    "        for h in self: h.remove()\n",
    "\n",
    "# example\n",
    "# with Hooks(model, append_stats) as hooks:\n",
    "#     learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Lambda(nn.Module): # Lambda layer\n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n",
    "    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),\n",
    "              GeneralRelu(**kwargs)]\n",
    "    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RunningBatchNorm(nn.Module):\n",
    "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        self.mults = nn.Parameter(torch.ones (nf,1,1))\n",
    "        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.register_buffer('sums', torch.zeros(1,nf,1,1))\n",
    "        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))\n",
    "        self.register_buffer('count', tensor(0.))\n",
    "        self.register_buffer('factor', tensor(0.))\n",
    "        self.register_buffer('offset', tensor(0.))\n",
    "        self.batch = 0\n",
    "\n",
    "    def update_stats(self, x):\n",
    "        bs,nc,*_ = x.shape\n",
    "        self.sums.detach_()\n",
    "        self.sqrs.detach_()\n",
    "        dims = (0,2,3)\n",
    "        s    = x    .sum(dims, keepdim=True)\n",
    "        ss   = (x*x).sum(dims, keepdim=True)\n",
    "        c    = s.new_tensor(x.numel()/nc)\n",
    "        mom1 = s.new_tensor(1 - (1-self.mom)/math.sqrt(bs-1))\n",
    "        self.sums .lerp_(s , mom1)\n",
    "        self.sqrs .lerp_(ss, mom1)\n",
    "        self.count.lerp_(c , mom1)\n",
    "        self.batch += bs\n",
    "        means = self.sums/self.count\n",
    "        varns = (self.sqrs/self.count).sub_(means*means)\n",
    "        if bool(self.batch < 20): varns.clamp_min_(0.01)\n",
    "        self.factor = self.mults / (varns+self.eps).sqrt()\n",
    "        self.offset = self.adds - means*self.factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training: self.update_stats(x)\n",
    "        return x*self.factor + self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_summary(model, data, find_all=False, print_mod=False):\n",
    "    xb,yb = get_batch(data.valid_dl, learn)\n",
    "    mods = find_modules(model, is_lin_layer) if find_all else model.children()\n",
    "    f = lambda hook,mod,inp,out: print(f\"====\\n{mod}\\n\" if print_mod else \"\", out.shape)\n",
    "    with Hooks(mods, f) as hooks: learn.model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Road map for a simple supervised learning problem\n",
    "### Preprocessing step\n",
    "1. Setup paths and download raw data\n",
    "2. Read the files and put data in a container (TextList object)\n",
    "    - TextList --> hold **all x data**\n",
    "3. Split data into train/valid (possibly test) set (SplitData object)\n",
    "    - SplitData --> contain **x data** split into train/valid sets\n",
    "4. Text preprocessing: cleaning up and tokenization (TokenizeProcessor)\n",
    "5. Numericalization (NumericalizeProcessor)\n",
    "6. (Supervised only) Labeling data --> (x,y) pairs (LabeledData object)\n",
    "    - Train/valid set each containing all **(x, y)** data, respectively\n",
    "    - Label should be done *after* splitting\n",
    "    - yields **SplitData holding LabeledList objects** for each train/valid sets\n",
    "    - For *LM tasks*, just use a dummy label for y now; (x,y) pair will be made in the batching step\n",
    "7. Make minibatches\n",
    "   (1) Prepare batches --> bs and bptt (LMPreLoader object)\n",
    "   (2) Batching for classification\n",
    "8. Dataloader and databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-tokenizing rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# NLP cleaning up text and definition of special tokens\n",
    "\n",
    "UNK = 'xxunk' # unknown\n",
    "PAD = 'xxpad'\n",
    "BOS = 'xxbos' # beginning of sentence\n",
    "EOS = 'xxeos' # end of sentence\n",
    "TK_REP = 'xxrep' # replace characters that repeated at least 4 times (e.g. aaaa) with the token\n",
    "# e.g. cccc --> xxrep 4 c\n",
    "TK_WREP = 'xxwrep' # replace repeated words with token\n",
    "# e.g. ha ha ha ha --> xxwrep 4 ha\n",
    "TK_UP = 'xxup' # ALL CAPS --> xxup all xxup caps\n",
    "TK_MAJ = 'xxmaj' # Capitialized Words --> xxmaj captialized xxmaj words\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replace <br /> by /n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>') # ? matches 0 or 1 time\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])',r' \\1 ', t) # \\1 is group 1\n",
    "def rm_useless_spaces(t):\n",
    "    'Remove multiple spaces'\n",
    "    return re.sub(' {2,}',' ',t) # {2,} means at least 2 repetition, no upper limit\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups() # e.g. if aaaa is in text, c=a, cc=aaa (one less than it should be)\n",
    "        return f' {TK_REP} {len(cc)+1} {c} ' # re counts 1 less\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})') # \\S is any non-whitespace character\n",
    "    return re_rep.sub(_replace_rep, t) # regex can pass function\n",
    "    \n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x): # html stuff\n",
    "    \"Various messy things we've seen in documents --> particularly in html\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "# these rules are functions to be passed to compose\n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. post-tokenizing rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "def add_eos_bos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parallel_nobar(func, arr, max_workers=4):\n",
    "    'Version that does NOT use fastai progress_bar class'\n",
    "    if max_workers < 2:\n",
    "        results = list(map(func, enumerate(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(ex.map(func, enumerate(arr)))\n",
    "    if any([o is not None for o in results]):\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# here is the fastai standard parallel function\n",
    "# note it uses fastprocess library\n",
    "\n",
    "'''If don't use fastprogress, don't export this and change the parallel func in __call__\n",
    "function in TokenizeProcessor to parallel_nobar'''\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    'Version that using fast.ai progress_bar class'\n",
    "    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "    if any([o is not None for o in results]): return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenizer\n",
    "- note this version uses the parallel function that uses fastprogress, a fastai libray. If don't want to do this, change the parallel to parallel_nobar in TokenizerProcessor `__call__` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH # Orth: The hash value of the lexeme (i.e. word)\n",
    "\n",
    "class TokenizeProcessor(Processor):\n",
    "    '''apply pre_rules, special_tokens, tokenizing and post_rules to\n",
    "    a list of texts'''\n",
    "    def __init__(self, lang='en', chunksize=2000, pre_rules=None,\n",
    "                post_rules=None, max_workers=4):\n",
    "        self.chunksize = chunksize\n",
    "        self.max_workers = max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer # here used Spacy tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}]) # what does ORTH do?\n",
    "        self.pre_rules = default_pre_rules if pre_rules is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "    \n",
    "    def __call__(self, items): # items must be a list or tuple of texts\n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): \n",
    "            items = [read_file(i) for i in items] # list of articles\n",
    "        # chunks are divided by number of articles, not content length\n",
    "        chunks = [items[i:i+self.chunksize] for i in range(0,len(items), self.chunksize)]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "#         toks = parallel_nobar(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, []) # combine token lists, sum(arr,[]) is a good trick to concat lists of lists\n",
    "        \n",
    "    def proc_chunk(self, args): # args is a list\n",
    "        i, chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk] # apply pre_rules\n",
    "        # tokenizing happens here\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)] # docs=list of token lists (each doc is a token list)\n",
    "        docs = [compose(t, self.post_rules) for t in docs] # apply post_rules\n",
    "        return docs\n",
    "    \n",
    "    def proc1(self,item):\n",
    "        'Process 1 item'\n",
    "        return self.proc_chunk([item])[0] # return list content b/c only 1 doc\n",
    "    def deprocess(self, toks):\n",
    "        'convert tokens back to a string'\n",
    "        return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):\n",
    "        'convert a token to a string'\n",
    "        return ' '.join(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumericalizeProcessor(Processor):\n",
    "    '''Turn tokens into numbers, set max_vocab and min_freq'''\n",
    "    def __init__(self, vocab = None, max_vocab = 60000, min_freq=2):\n",
    "        self.vocab = vocab\n",
    "        self.max_vocab = max_vocab\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def __call__(self, items): # items are token lists\n",
    "        # Vocab defined at first use\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o) # loop through all docs and all tokens in each doc\n",
    "            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c>= self.min_freq] # build vocab from high to low frequency words\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: # remove special tokens from the vocab\n",
    "                    self.vocab.remove(o)\n",
    "                self.vocab.insert(0,o) # insert special tokens back to the beginning following the original order\n",
    "        \n",
    "        if getattr(self,'otoi', None) is None:\n",
    "            # build reverse dict, \n",
    "            self.otoi = defaultdict(int,{v:k for k,v in enumerate(self.vocab)})\n",
    "        # build vocab done above\n",
    "        return [self.proc1(o) for o in items] # process one doc at a time\n",
    "    \n",
    "    def proc1(self, item):\n",
    "        # label by index (higher to lower frequency except special tokens)\n",
    "        return [self.otoi[o] for o in item] # process one document/sentence\n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(i) for i in idxs]\n",
    "    def deproc1(self,idx):\n",
    "        return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 For language modeling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Example:\\nproc_tok= TokenizeProcessor(max_workers=8)\\nproc_num = NumericalizeProcessor()\\nll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Example:\n",
    "proc_tok= TokenizeProcessor(max_workers=8)\n",
    "proc_num = NumericalizeProcessor()\n",
    "ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LM_PreLoader():\n",
    "    'prepare next word prediction (x, y) batches'\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data = data\n",
    "        self.bs = bs # number of sentences to work on at the same time\n",
    "        self.bptt = bptt # backprop through time = # of tokens our RNN will backprop through before it's forgotten \n",
    "        self.shuffle = shuffle\n",
    "        # --> batch shape = (bs, bptt)\n",
    "        total_len = sum([len(t) for t in data.x]) # total number of tokens in all data\n",
    "        self.n_batch = total_len//bs # number of tokens in a \"sentence (batch)\"\n",
    "        self.batchify() #\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.x # texts are already token indices\n",
    "        if self.shuffle:\n",
    "            texts = texts[torch.randperm(len(texts))] # shuffle\n",
    "        stream = torch.cat([torch.tensor(t) for t in texts]) # turn token indices into tensors\n",
    "        self.batched_data = stream[:self.n_batch*self.bs].view(self.bs, self.n_batch) # total_len ~= n_batch * bs, this is to be divided again when getitem\n",
    "        \n",
    "    def __len__(self): # total number of batches\n",
    "        return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''returns (x,y) pair where y is the next word of x\n",
    "        '''\n",
    "        source = self.batched_data[idx % self.bs] # row number in a batch\n",
    "        seq_idx = (idx //self.bs) * self.bptt # ???? to be understood\n",
    "        return source[seq_idx:seq_idx+self.bptt], source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'(x,y) batch maker for LM task (predict next word)\\n Sentence length = {self.n_batch}'\n",
    "\n",
    "# convenience functions for language model tasks\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))\n",
    "\n",
    "# Example:\n",
    "# data = lm_databunchify(ll, bs, bptt) # all raw data (ll = LabeledList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 For classification tasks (x = texts, y = labels, e.g. sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Sampler\n",
    "\n",
    "class SortSampler(Sampler): # for validation set\n",
    "    ''' Get indices of docs that is reverse-sorted by key (e.g. get \n",
    "    indices of the documents from longest to shortest)'''\n",
    "    def __init__(self, data_source, key):\n",
    "        self.data_source = data_source\n",
    "        self.key = key\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    def __iter__(self): # feed longest text first, return indices of the correspond texts\n",
    "        return iter(sorted(list(range(len(self.data_source))), key = self.key, reverse=True))\n",
    "\n",
    "# ??? needs to read more carefully\n",
    "class SortishSampler(Sampler):\n",
    "    '''\n",
    "    Note: key is a callable function\n",
    "    '''\n",
    "    def __init__(self, data_source,key, bs):\n",
    "        self.data_source = data_source\n",
    "        self.key = key\n",
    "        self.bs = bs\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)] # 50 times bigger megabatch\n",
    "        sorted_idx = torch.cat([torch.tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches]) # reverse-sort the megabatches by key\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)] # extract batch indices from megabatch indices\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0], batches[max_idx] = batches[max_idx],batches[0] # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2) #excluding begin and end\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else torch.LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    ''' '''\n",
    "    max_len = max([len(s[0]) for s in samples]) # s[0] is x, s[1] is y\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx # a giant matrix with all elements = pad_idx\n",
    "    for i, s in enumerate(samples):\n",
    "        if pad_first:\n",
    "            res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:\n",
    "            res[i, :len(s[0])] = torch.LongTensor(s[0])\n",
    "    return res, torch.tensor([s[1] for s in samples]) # (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package sampler and padding together for classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this line if need to be exported\n",
    "#export\n",
    "class MakeRGB(Transform):\n",
    "    def __call__(self, item): return item.convert('RGB')\n",
    "\n",
    "def make_rgb(item): return item.convert('RGB')\n",
    "\n",
    "class ResizeFixed(Transform):\n",
    "    _order=10\n",
    "    def __init__(self,size):\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)\n",
    "\n",
    "def to_byte_tensor(item):\n",
    "    res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))\n",
    "    w,h = item.size\n",
    "    return res.view(h,w,-1).permute(2,0,1)\n",
    "to_byte_tensor._order=20\n",
    "\n",
    "def to_float_tensor(item): return item.float().div_(255.)\n",
    "to_float_tensor._order=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this line if need to be exported\n",
    "#export\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this line if need to be exported\n",
    "#export\n",
    "def noop(x): return x\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride=1):\n",
    "        super().__init__()\n",
    "        nf,ni = nh*expansion,ni*expansion\n",
    "        layers  = [conv_layer(ni, nh, 3, stride=stride),\n",
    "                   conv_layer(nh, nf, 3, zero_bn=True, act=False)\n",
    "        ] if expansion == 1 else [\n",
    "                   conv_layer(ni, nh, 1),\n",
    "                   conv_layer(nh, nh, 3, stride=stride),\n",
    "                   conv_layer(nh, nf, 1, zero_bn=True, act=False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n",
    "    \n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in+1)*8, 64, 64]\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "            *stem,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *res_layers,\n",
    "            nn.AdaptiveAvgPool2d(1), Flatten(),\n",
    "            nn.Linear(nfs[-1]*expansion, c_out),\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride):\n",
    "        return nn.Sequential(\n",
    "            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "              for i in range(n_blocks)])\n",
    "    \n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)\n",
    "def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this line if need to be exported\n",
    "#export\n",
    "def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,\n",
    "                lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):\n",
    "    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)\n",
    "    if progress: cbfs.append(ProgressCallback)\n",
    "    if cuda:     cbfs.append(CudaCallback)\n",
    "    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))\n",
    "    if mixup:    cbfs.append(partial(MixUp, mixup))\n",
    "    arch_args = {}\n",
    "    if not c_in : c_in  = data.c_in\n",
    "    if not c_out: c_out = data.c_out\n",
    "    if c_in:  arch_args['c_in' ]=c_in\n",
    "    if c_out: arch_args['c_out']=c_out\n",
    "    return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past unused versions, do NOT export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.lrs = [[] for _ in self.opt.param_groups]\n",
    "        self.losses = []\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        for pg,lr in zip(self.opt.param_groups,self.lrs): \n",
    "            lr.append(pg['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "\n",
    "    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n",
    "    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n",
    "\n",
    "    def plot(self, skip_last=0, pgid=-1):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        lrs    = self.lrs[pgid]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(lrs[:n], losses[:n])\n",
    "\n",
    "# old version\n",
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n",
    "\n",
    "    def begin_fit(self):\n",
    "        if not isinstance(self.sched_funcs, (list,tuple)):\n",
    "            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n",
    "\n",
    "    def set_param(self):\n",
    "        assert len(self.opt.param_groups)==len(self.sched_funcs)\n",
    "        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n",
    "            pg[self.pname] = f(self.n_epochs/self.epochs)\n",
    "\n",
    "    def begin_batch(self):\n",
    "        if self.in_train: self.set_param()\n",
    "\n",
    "# old oversion\n",
    "class LR_Find(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def begin_batch(self):\n",
    "        if not self.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
    "        for pg in self.opt.param_groups: pg['lr'] = lr\n",
    "\n",
    "    def after_step(self):\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss: self.best_loss = self.loss\n",
    "\n",
    "#old version\n",
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data\n",
    "# old version\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        self.in_train = False\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "    @property\n",
    "    def opt(self):       return self.learn.opt\n",
    "    @property\n",
    "    def model(self):     return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self):      return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        try:\n",
    "            self.xb,self.yb = xb,yb\n",
    "            self('begin_batch')\n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            if not self.in_train: return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException: self('after_cancel_batch')\n",
    "        finally: self('after_batch')\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        try:\n",
    "            for xb,yb in dl: self.one_batch(xb, yb)\n",
    "        except CancelEpochException: self('after_cancel_epoch')\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            self('begin_fit')\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                self('after_epoch')\n",
    "\n",
    "        except CancelTrainException: self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
    "        return res\n",
    "\n",
    "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):\n",
    "    if opt_func is None: opt_func = optim.SGD\n",
    "    opt = opt_func(model.parameters(), lr=lr)\n",
    "    learn = Learner(model, opt, loss_func, data)\n",
    "    return learn, Runner(cb_funcs=listify(cbs))\n",
    "\n",
    "# less-old version\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        #We use the logger function of the `Learner` here, it can be customized to write in a file or in a progress bar\n",
    "        self.logger(self.train_stats)\n",
    "        self.logger(self.valid_stats) \n",
    "        \n",
    "#oldest version\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "\n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)\n",
    "        \n",
    "def init_cnn_(m, f):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        f(m.weight, a=0.1)\n",
    "        if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "    for l in m.children(): init_cnn_(l, f)\n",
    "\n",
    "def init_cnn(m, uniform=False):\n",
    "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
    "    init_cnn_(m, f)\n",
    "\n",
    "def get_batch(dl, run):\n",
    "    run.xb,run.yb = next(iter(dl))\n",
    "    for cb in run.cbs: cb.set_runner(run)\n",
    "    run('begin_batch')\n",
    "    return run.xb,run.yb\n",
    "def model_summary(run, learn, data, find_all=False):\n",
    "    xb,yb = get_batch(data.valid_dl, run)\n",
    "    device = next(learn.model.parameters()).device#Model may not be on the GPU yet\n",
    "    xb,yb = xb.to(device),yb.to(device)\n",
    "    mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()\n",
    "    f = lambda hook,mod,inp,out: print(f\"{mod}\\n{out.shape}\\n\")\n",
    "    with Hooks(mods, f) as hooks: learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted XLibrary_Lesson6.ipynb to exp/nb_XLibrary.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py XLibrary_Lesson6.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
