{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastai import datasets as FA_dataset\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data from previous file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_Matmul import MNIST_URL, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://deeplearning.net/data/mnist/mnist.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/xianli/Desktop/fast/Part2/data/mnist.pkl.gz')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensors(url,dest):\n",
    "    path = FA_dataset.download_data(url, dest, ext='.gz')\n",
    "    with gzip.open(path,'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    return (data-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_tensors(MNIST_URL,fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean,train_std)\n",
    "# note: must use training set mean and std\n",
    "x_valid = normalize(x_valid, train_mean,train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.061423922190443e-05, tensor(1.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean().item(), x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0058), tensor(0.9924))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.mean(),x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = y_train.unique().numel() # number of classes\n",
    "n,m = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50 # number of hidden units\n",
    "fan_in = 784 # input units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple random initialization for a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(fan_in,nh)/math.sqrt(fan_in)\n",
    "b1 = torch.zeros(nh)/math.sqrt(nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.0170e-05), tensor(0.0355))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x,w,b): return x@w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = lin(x_valid,w1,b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0092), tensor(1.0014))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.mean(), out1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(y): return y.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = relu(lin(x_valid,w1,b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3992), tensor(0.6016))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean and std of the activations are NOT 0 and 1 due to relu cutting off the negative values\n",
    "x2.mean(), x2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaiming initialization for relu linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(fan_in,nh)*math.sqrt(2./fan_in)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,c)*math.sqrt(2./nh)\n",
    "b2 = torch.zeros(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.2457e-05), tensor(0.0505))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(),w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4931), tensor(0.7554), torch.Size([10000, 50]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = relu(lin(x_valid,w1,b1))\n",
    "x1.mean(),x1.std(),x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5102), tensor(0.7362))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 =relu(lin(x1,w2,b2))\n",
    "x2.mean(),x2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch version of Kaiming initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0547, -0.0208,  0.0193,  ...,  0.0275, -0.0175, -0.0357],\n",
       "        [ 0.0218, -0.0213,  0.0158,  ...,  0.0830,  0.0357,  0.0437],\n",
       "        [-0.0119, -0.0219,  0.0352,  ...,  0.0522,  0.0225,  0.0090],\n",
       "        ...,\n",
       "        [-0.0388,  0.0043, -0.0166,  ...,  0.0049, -0.0482,  0.0159],\n",
       "        [-0.0433,  0.0127,  0.0765,  ...,  0.1420, -0.0131,  0.0721],\n",
       "        [-0.0116,  0.0465,  0.0008,  ...,  0.0268,  0.0171, -0.0060]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.zeros(fan_in,nh)\n",
    "init.kaiming_normal_(w1,mode = 'fan_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "??init.kaiming_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0001), tensor(0.0505))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = relu(lin(x_valid,w1,b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5276), tensor(0.8088))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.mean(), x1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcc(input, *args, fan_out=10):\n",
    "    n, p = input.shape\n",
    "    l_sizes = [p]+args[0]\n",
    "    print(l_sizes)\n",
    "    w = []\n",
    "    b = []\n",
    "    for i in range(1,len(l_sizes)):\n",
    "            w.append(init.kaiming_normal_(torch.zeros(l_sizes[i-1],l_sizes[i]),mode='fan_out'))\n",
    "            b.append(torch.zeros(l_sizes[i]))\n",
    "    w.append(init.kaiming_normal_(torch.zeros(l_sizes[-1],fan_out),mode='fan_out'))\n",
    "    b.append(torch.zeros(fan_out))\n",
    "    x = input\n",
    "    for i in range(len(w)):\n",
    "        x = relu(lin(x,w[i],b[i]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 50, 100, 75]\n"
     ]
    }
   ],
   "source": [
    "nh=[50,100,75]\n",
    "out = fcc(x_valid,nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3661), tensor(0.5288))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.mean(),out.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 50, 100, 75]\n",
      "CPU times: user 38.8 ms, sys: 2.57 ms, total: 41.3 ms\n",
      "Wall time: 26.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.7566, 0.0000,  ..., 2.0485, 0.4644, 0.0684],\n",
       "        [0.0000, 1.1972, 0.0000,  ..., 1.5445, 0.4188, 0.9135],\n",
       "        [0.8977, 1.0240, 0.7300,  ..., 0.6599, 0.5144, 0.3513],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.8619,  ..., 0.3754, 0.0244, 0.0000],\n",
       "        [0.3691, 0.8045, 0.0000,  ..., 1.1513, 0.9783, 0.0000],\n",
       "        [0.6838, 0.5602, 0.0000,  ..., 1.3448, 0.2002, 0.0000]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fcc(x_valid,nh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers as classes, basic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w # dim = (p,q)\n",
    "        self.b = b # dim = (q)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        self.x = x # dim = (n,p)\n",
    "        self.out = x@self.w +self.b # dim = (n,q)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        # important: every grad is scalar loss function wrt the matrix/vector\n",
    "        self.x.g = self.out.g @ self.w.t() # dim = (n,q) @ (q, p) = (n,p) \n",
    "        # w.g involves batch operation --> one more rank\n",
    "        # work by using broadcasting\n",
    "        self.w.g = (self.x.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(dim=0)# dim = (n,p,1) * (n,1,q) --> (n,p,q) -> sum(0) --> (q,p)\n",
    "        self.b.g = self.out.g.sum(0) # dim = (n,q).sum(0) --> (q)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g * (self.inp>0.).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp # dim = (n,1)\n",
    "        self.targ = targ.float() # dim = (n)\n",
    "#         print('MSE: self.inp.shape = ', inp.shape)\n",
    "#         print('MSE: self.targ.shape = ', targ.shape)\n",
    "        return (self.targ-inp.squeeze()).pow(2).sum(0)/ inp.shape[0]\n",
    "    def backward(self):\n",
    "        self.inp.g = 2* (self.inp.squeeze() - self.targ).unsqueeze(-1)/self.inp.shape[0] # dim = (n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(): # forward pass calculate loss, backward calculate gradient\n",
    "    def __init__(self, w1,b1,w2,b2):\n",
    "        self.layers = [Lin(w1,b1), ReLu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "    \n",
    "    def __call__(self, x, targ): # this model will be called as function\n",
    "        for l in self.layers:\n",
    "            # the call inputs for each layer is the same. note each l is an instance of the classes\n",
    "            # __call__ functions will be used\n",
    "            x = l(x)\n",
    "        return self.loss(x,targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward() # this instance has stored the values of x and targ --> each layer will modify self.x or self.w that itself stores\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in = 784\n",
    "nh = 50\n",
    "c = 1 # only one class because MSE loss can't work for multiclass problem\n",
    "\n",
    "w1 = torch.randn(fan_in,nh)*math.sqrt(2./fan_in)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,c)*math.sqrt(2./nh)\n",
    "b2 = torch.zeros(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 7.45 ms, total: 124 ms\n",
      "Wall time: 68.9 ms\n",
      "MSE loss =  25.117624282836914\n"
     ]
    }
   ],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None] * 4 # need to create the property first\n",
    "model = Model(w1,b1,w2,b2)\n",
    "%time loss = model(x_train,y_train) # this is one train cycle, no SDG yet\n",
    "print('MSE loss = ', loss.item())\n",
    "# model.backward() # automatically change all parameter gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 s, sys: 24.1 s, total: 32.2 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%time model.backward() # very very slow backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare to correct results by pytorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 350 ms, sys: 87.8 ms, total: 437 ms\n",
      "Wall time: 157 ms\n",
      "tensor(25.1176, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)\n",
    "\n",
    "def mse(y_hat, y):\n",
    "    return (y_hat.squeeze()-y.float()).pow(2).mean()\n",
    "\n",
    "def forward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    # we don't actually need the loss in backward!\n",
    "    return mse(out, targ)\n",
    "\n",
    "loss_2 = forward(xt2,y_train)\n",
    "%time loss_2.backward() # pytorch autograd\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        ...,\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828]])\n",
      "tensor([[-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        ...,\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828],\n",
      "        [-0.1630, -0.1589,  0.0414,  ...,  0.0216, -0.1592, -0.0828]])\n"
     ]
    }
   ],
   "source": [
    "print(w12.grad)\n",
    "print(w1.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(w12.grad, w1.g, rtol = 1e-3)\n",
    "torch.allclose(w22.grad, w2.g, rtol = 1e-3)\n",
    "torch.allclose(b12.grad, b1.g, rtol = 1e-3)\n",
    "torch.allclose(b22.grad, b2.g, rtol = 1e-3)\n",
    "torch.allclose(xt2.grad, x_train.g, rtol = 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor as a module (important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outsource all the __call__ function to the base module\n",
    "# make sure to understand this structure\n",
    "class myModule():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args # self.args is a tuple \n",
    "        print('mymodule called')\n",
    "        self.out = self.forward(*args) # overriding method\n",
    "        print('mymodule finish child forward call')\n",
    "        return self.out # report loss from here\n",
    "    \n",
    "    def forward(self): # this is always overridden\n",
    "        raise ValueError('Forward not implemented in root module!')\n",
    "        \n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args) # bwd method to be implemented by the child modules \n",
    "        # will automatically get the output of the forward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin2(myModule):\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w # dim = (p,q)\n",
    "        self.b = b # dim = (q)\n",
    "        print('Lin2 class initializes')\n",
    "    \n",
    "    def forward(self,x):\n",
    "        print('Lin2 forward called')\n",
    "        return x@self.w +self.b # dim = (n,q)\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        # important: every grad is scalar loss function wrt the matrix/vector\n",
    "        inp.g = out.g @ self.w.t() # dim = (n,q) @ (q, p) = (n,p) \n",
    "        # replaced by einsum\n",
    "#         self.w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(dim=0)# main reason for the slowness\n",
    "        self.w.g = torch.einsum('ki,kj->ij', inp, out.g)# dim = (n,p,1) * (n,1,q) --> (n,p,q) -> sum(0) --> (q,p)\n",
    "        self.b.g = out.g.sum(0) # dim = (n,q).sum(0) --> (q)\n",
    "\n",
    "class ReLu(myModule):\n",
    "    def forward(self, inp):\n",
    "        print('Call ReLu forward')\n",
    "        return inp.clamp_min(0.)\n",
    "    \n",
    "    def bwd(self,out,inp):\n",
    "        inp.g = out.g * (inp>0.).float()\n",
    "\n",
    "# loss function\n",
    "class Mse(myModule):\n",
    "    def forward(self, inp, targ):\n",
    "        print('Call MSE forward')\n",
    "        return (targ.float()-inp.squeeze()).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ):\n",
    "        inp.g = 2* (inp.squeeze() - targ.float()).unsqueeze(-1)/inp.shape[0] # dim = (n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2():\n",
    "    def __init__(self,w1,b1,w2,b2):\n",
    "        self.layers= [Lin2(w1,b1), ReLu(), Lin2(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in = 784\n",
    "nh = 50\n",
    "c = 1 # only one class because MSE loss can't work for multiclass problem\n",
    "\n",
    "w1 = torch.randn(fan_in,nh)*math.sqrt(2./fan_in)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,c)*math.sqrt(2./nh)\n",
    "b2 = torch.zeros(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lin2 class initializes\n",
      "Lin2 class initializes\n",
      "mymodule called\n",
      "Lin2 forward called\n",
      "mymodule finish child forward call\n",
      "mymodule called\n",
      "Call ReLu forward\n",
      "mymodule finish child forward call\n",
      "mymodule called\n",
      "Lin2 forward called\n",
      "mymodule finish child forward call\n",
      "mymodule called\n",
      "Call MSE forward\n",
      "mymodule finish child forward call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(21.9231)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Model2(w1,b1,w2,b2)\n",
    "model2(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 79 ms, total: 331 ms\n",
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    "%time model2.backward() # wall time = 30 s if not using einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, dim_in, nh, dim_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(dim_in,nh), nn.ReLU(), nn.Linear(nh,dim_out)]\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3075, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_in = 784\n",
    "nh=50\n",
    "dim_out = 10\n",
    "torch_model = model(dim_in,nh,dim_out)\n",
    "loss = torch_model(x_train, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 147 ms, sys: 18.4 ms, total: 165 ms\n",
      "Wall time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
