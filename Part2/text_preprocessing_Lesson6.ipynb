{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_XLibrary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import datasets as FAdatasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road map for a simple supervised learning problem\n",
    "### Preprocessing step\n",
    "1. Setup paths and download raw data\n",
    "2. Read the files and put data in a container (TextList object)\n",
    "    - TextList --> hold **all x data**\n",
    "3. Split data into train/valid (possibly test) set (SplitData object)\n",
    "    - SplitData --> contain **x data** split into train/valid sets\n",
    "4. Text preprocessing: cleaning up and tokenization (TokenizeProcessor)\n",
    "5. Numericalization (NumericalizeProcessor)\n",
    "6. (Supervised only) Labeling data --> (x,y) pairs (LabeledData object)\n",
    "    - Train/valid set each containing all **(x, y)** data, respectively\n",
    "    - Label should be done *after* splitting\n",
    "    - yields **SplitData holding LabeledList objects** for each train/valid sets\n",
    "    - For *LM tasks*, just use a dummy label for y now; (x,y) pair will be made in the batching step\n",
    "7. Make minibatches\n",
    "   (1) Prepare batches --> bs and bptt (LMPreLoader object)\n",
    "   (2) Batching for classification\n",
    "8. Dataloader and databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Path and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/xianli/Desktop/fast/Part2/data')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir = Path('.').resolve()\n",
    "data_dir = home_dir/'data'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = FAdatasets.untar_data(FAdatasets.URLs.IMDB, dest=data_dir)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/tmp_clas'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/imdb.vocab'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/unsup'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/README'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/tmp_lm'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/train')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read data into a TextList container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_file(fn):\n",
    "    with open(fn,'r', encoding='utf8') as f:\n",
    "        return f.read() # this is reading all the contents\n",
    "    \n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "        return cls(get_files(path, extensions,recurse=recurse,include=include), path, **kwargs)\n",
    "    # get_files is a standalone function that return all file paths in all folders\n",
    "    # the second 'path' is for the parent ItemList initialization\n",
    "    # Note the entire get_files(path, extensions,recurse=recurse,include=include)\n",
    "    # is items input in the ItemList class initialization\n",
    "    \n",
    "    def get(self,i):\n",
    "        # overload parent get method --> show how to access individual data\n",
    "        if isinstance(i, Path):\n",
    "            return read_file(i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = TextList.from_files(file_path, include=['train','test','unsup']) # concat together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_list.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\",\n",
       " 'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.',\n",
       " 'In Los Angeles, the alcoholic and lazy Hank Chinaski (Matt Dillon) performs a wide range of non-qualified functions just to get enough money to drink and gamble in horse races. His primary and only objective is writing and having sexy with dirty women.<br /><br />\"Factotum\" is an uninteresting, pointless and extremely boring movie about an irresponsible drunken vagrant that works a couple of days or weeks just to get enough money to buy spirits and gamble, being immediately fired due to his reckless behavior. In accordance with IMDb, this character would be the fictional alter-ego of the author Charles Bukowski, and based on this story, I will certainly never read any of his novels. Honestly, if the viewer likes this theme of alcoholic couples, better off watching the touching and heartbreaking Hector Babenco\\'s \"Ironweed\" or Marco Ferreri\\'s \"Storie di Ordinaria Follia\" that is based on the life of the same writer. My vote is four.<br /><br />Title (Brazil): \"Factotum \\x96 Sem Destino\" (\"Factotum \\x96 Without Destiny\")']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __getitem__ work flow:\n",
    "# 1. root class list_container returns the item (a file path in here) corresponding to the index\n",
    "# 2. the item(s) then go through ItemList private _get method and take any transformation provided\n",
    "# 3. public get method is called and use the TextList get method to read the item (i.e. file path)\n",
    "txt = item_list[0:3]\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split data for language modeling (no label, so can use all data)\n",
    "- use all texts and leave 10% behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# must return booleans\n",
    "def random_splitter(fn, p_valid): return random.random() < p_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (89945 items)\n",
       "[PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/1821_4.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/9487_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/4604_4.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/2828_2.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/10890_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/3351_4.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/8070_2.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/1027_4.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/8248_3.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/4290_4.txt')...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb\n",
       "Valid: TextList (10055 items)\n",
       "[PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/10096_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/6648_2.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/2760_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/7900_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/1082_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/12059_1.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/3583_3.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/2813_4.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/11377_4.txt'), PosixPath('/Users/xianli/Desktop/fast/Part2/data/imdb/test/neg/7958_4.txt')...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this split_by_func works by assigning True/False to each item according to the provided function\n",
    "sd = SplitData.split_by_func(item_list, partial(random_splitter, p_valid=0.1))\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# html here is just to clean up HTML stuff\n",
    "import spacy, html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-tokenizing rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# special tokens to replace original texts\n",
    "from typing import Collection\n",
    "\n",
    "UNK = 'xxunk' # unknown\n",
    "PAD = 'xxpad'\n",
    "BOS = 'xxbos' # beginning of sentence\n",
    "EOS = 'xxeos' # end of sentence\n",
    "TK_REP = 'xxrep' # replace characters that repeated at least 4 times (e.g. aaaa) with the token\n",
    "# e.g. cccc --> xxrep 4 c\n",
    "TK_WREP = 'xxwrep' # replace repeated words with token\n",
    "# e.g. ha ha ha ha --> xxwrep 4 ha\n",
    "TK_UP = 'xxup' # ALL CAPS --> xxup all xxup caps\n",
    "TK_MAJ = 'xxmaj' # Capitialized Words --> xxmaj captialized xxmaj words\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replace <br /> by /n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>') # ? matches 0 or 1 time\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])',r' \\1 ', t) # \\1 is group 1\n",
    "def rm_useless_spaces(t):\n",
    "    'Remove multiple spaces'\n",
    "    return re.sub(' {2,}',' ',t) # {2,} means at least 2 repetition, no upper limit\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups() # e.g. if aaaa is in text, c=a, cc=aaa (one less than it should be)\n",
    "        return f' {TK_REP} {len(cc)+1} {c} ' # re counts 1 less\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})') # \\S is any non-whitespace character\n",
    "    return re_rep.sub(_replace_rep, t) # regex can pass function\n",
    "    \n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents --> particularly in html\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "# these rules are functions to be passed to compose\n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'aaa'), ('c', 'cccc')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_wrep = re.compile(r'(\\S)(\\1{3,})')\n",
    "a = 'b aaaa ddd ccccc'\n",
    "out = re_wrep.findall(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-tokenizing rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "def add_eos_bos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    'Version that using fast.ai progress_bar class'\n",
    "    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "    if any([o is not None for o in results]): return results\n",
    "\n",
    "def parallel_nobar(func, arr, max_workers=4):\n",
    "    'Version that does NOT use fastai progress_bar class'\n",
    "    if max_workers < 2:\n",
    "        results = list(map(func, enumerate(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(ex.map(func, enumerate(arr)))\n",
    "    if any([o is not None for o in results]):\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH # Orth: The hash value of the lexeme (i.e. word)\n",
    "\n",
    "class TokenizeProcessor(Processor):\n",
    "    '''apply pre_rules, special_tokens, tokenizing and post_rules to\n",
    "    a list of texts'''\n",
    "    def __init__(self, lang='en', chunksize=2000, pre_rules=None,\n",
    "                post_rules=None, max_workers=4):\n",
    "        self.chunksize = chunksize\n",
    "        self.max_workers = max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer # here used Spacy tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}]) # what does ORTH do?\n",
    "        self.pre_rules = default_pre_rules if pre_rules is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "    \n",
    "    def __call__(self, items): # items must be a list or tuple of texts\n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): \n",
    "            items = [read_file(i) for i in items] # list of articles\n",
    "        # chunks are divided by number of articles, not content length\n",
    "        chunks = [items[i:i+self.chunksize] for i in range(0,len(items), self.chunksize)]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "#         toks = parallel_nobar(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, []) # combine token lists, sum(arr,[]) is a good trick to concat lists of lists\n",
    "        \n",
    "    def proc_chunk(self, args): # args is a list\n",
    "        i, chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk] # apply pre_rules\n",
    "        # tokenizing happens here\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)] # docs=list of token lists (each doc is a token list)\n",
    "        docs = [compose(t, self.post_rules) for t in docs] # apply post_rules\n",
    "        return docs\n",
    "    \n",
    "    def proc1(self,item):\n",
    "        'Process 1 item'\n",
    "        return self.proc_chunk([item])[0] # return list content b/c only 1 doc\n",
    "    def deprocess(self, toks):\n",
    "        'convert tokens back to a string'\n",
    "        return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):\n",
    "        'convert a token to a string'\n",
    "        return ' '.join(tok)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1, 2], [3, 4], [5, 6]]\n",
    "sum(a, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\",\n",
       "  'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.'],\n",
       " 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = item_list[0:2]\n",
    "test_text, len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'xxbos|i|have|seen|this|movie|and|i|did|not|care|for|this|movie|anyhow|.|i|would|not|think|about|going|to|xxmaj|paris|because|i|do|not|like|this|country|and|its|national|capital|.|i|do|not|like|to|learn|french|anyhow|because|i|do|not|understand|their|language|.|xxmaj|why|would|i|go|to|xxmaj|france|when|i|rather|go|to|xxmaj|germany|or|the|xxmaj|united|xxmaj|kingdom|?|xxmaj|germany|and|the|xxmaj|united|xxmaj|kingdom|are|the|nations|i|tolerate|.|xxmaj|apparently|the|xxmaj|olsen|xxmaj|twins|do|not|understand|the|xxmaj|french|language|just|like|me|.|xxmaj|therefore|i|will|not|bother|the|xxmaj|france|trip|no|matter|what|.|i|might|as|well|stick|to|the|xxmaj|united|xxmaj|kingdom|and|meet|single|women|and|play|video|games|if|there|is|a|video|arcade|.|xxmaj|that|is|all|.|xxeos'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = TokenizeProcessor()\n",
    "'|'.join(tp(test_text)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tp(test_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test nobar version parallel (change just one line to parallel_nobar)\n",
    "tp = TokenizeProcessor()\n",
    "'|'.join(tp(test_text)[1]) # works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    '''Turn tokens into numbers, set max_vocab and min_freq'''\n",
    "    def __init__(self, vocab = None, max_vocab = 60000, min_freq=2):\n",
    "        self.vocab = vocab\n",
    "        self.max_vocab = max_vocab\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def __call__(self, items): # items are token lists\n",
    "        # Vocab defined at first use\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o) # loop through all docs and all tokens in each doc\n",
    "            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c>= self.min_freq] # build vocab from high to low frequency words\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: # remove special tokens from the vocab\n",
    "                    self.vocab.remove(o)\n",
    "                self.vocab.insert(0,o) # insert special tokens back to the beginning following the original order\n",
    "        \n",
    "        if getattr(self,'otoi', None) is None:\n",
    "            # build reverse dict, \n",
    "            self.otoi = defaultdict(int,{v:k for k,v in enumerate(self.vocab)})\n",
    "        # build vocab done above\n",
    "        return [self.proc1(o) for o in items] # process one doc at a time\n",
    "    \n",
    "    def proc1(self, item):\n",
    "        # label by index (higher to lower frequency except special tokens)\n",
    "        return [self.otoi[o] for o in item] # process one document/sentence\n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(i) for i in idxs]\n",
    "    def deproc1(self,idx):\n",
    "        return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For LM task only\n",
    "- When we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare processors\n",
    "proc_tok= TokenizeProcessor(max_workers=8)\n",
    "proc_num = NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45' class='' max='45', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [45/45 01:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 00:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lambda x: 0 is just a dummy label because we don't need it\n",
    "'''Note in the process, we keep the good practice that we label\n",
    "using the train data and use the same labels for validation set\n",
    "\n",
    "We have ~90,000 docs/texts for training set ==> 45-46 chunks\n",
    "~ 10,000 docs for validation set ==> 5-6 chunks\n",
    "'''\n",
    "ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: LabeledData\n",
       "x: TextList (90084 items)\n",
       "[[2, 7, 1837, 7, 9790, 205, 7, 3680, 7, 4334, 220, 67, 374, 27, 2455, 124, 7, 183, 7, 5533, 2613, 17, 19, 1216, 508, 9, 7, 16, 22, 295, 334, 28, 80, 36164, 11, 7, 9790, 22, 150, 27, 7, 4241, 7, 17590, 9, 7, 151, 297, 171, 0, 14, 2825, 199, 74, 520, 64, 8, 29, 10, 16, 209, 9, 7, 8, 130, 28, 8, 29, 296, 1045, 10, 30, 43, 240, 8, 1468, 17, 168, 8, 31, 20, 16, 25, 37, 2001, 141, 26, 88, 26, 16, 95, 41, 99, 9, 7, 8, 211, 20, 16, 15, 16906, 47, 12, 271, 2728, 3383, 11, 680, 2728, 12154, 83, 44736, 200, 9, 7, 8, 29, 15, 295, 12, 0, 63, 28, 178, 68, 93, 439, 374, 47, 7, 9790, 10, 7, 4334, 10, 11, 7, 17590, 9, 3], [2, 18, 41, 131, 19, 29, 11, 18, 87, 37, 476, 28, 19, 29, 6730, 9, 18, 73, 37, 122, 59, 182, 14, 7, 1492, 107, 18, 58, 37, 53, 19, 664, 11, 112, 2028, 5847, 9, 18, 58, 37, 53, 14, 868, 704, 6730, 107, 18, 58, 37, 409, 80, 1076, 9, 7, 153, 73, 18, 159, 14, 7, 2113, 69, 18, 271, 159, 14, 7, 2530, 55, 8, 7, 2237, 7, 4695, 66, 7, 2530, 11, 8, 7, 2237, 7, 4695, 38, 8, 6063, 18, 7861, 9, 7, 679, 8, 7, 8131, 7, 3566, 58, 37, 409, 8, 7, 704, 1076, 57, 53, 89, 9, 7, 1583, 18, 105, 37, 1326, 8, 7, 2113, 1277, 74, 520, 64, 9, 18, 251, 26, 88, 1334, 14, 8, 7, 2237, 7, 4695, 11, 913, 689, 365, 11, 317, 397, 1488, 63, 54, 15, 12, 397, 17153, 9, 7, 20, 15, 44, 9, 3], [2, 7, 17, 7, 2927, 7, 3384, 10, 8, 4677, 11, 3029, 7, 5269, 7, 27592, 36, 7, 2201, 7, 7187, 33, 5083, 12, 1901, 2046, 13, 700, 23, 10612, 9962, 57, 14, 98, 213, 304, 14, 2716, 11, 12155, 17, 1708, 5395, 9, 7, 40, 4186, 11, 82, 5296, 15, 515, 11, 280, 1248, 27, 1584, 365, 9, 24, 21, 7, 24470, 21, 15, 48, 2728, 10, 1145, 11, 578, 368, 29, 59, 48, 8856, 3820, 32173, 20, 538, 12, 384, 13, 499, 55, 2217, 57, 14, 98, 213, 304, 14, 810, 4762, 11, 12155, 10, 128, 1240, 3842, 711, 14, 40, 9523, 1962, 9, 7, 17, 23003, 27, 946, 10, 19, 123, 73, 42, 8, 3081, 5644, 23, 3875, 13, 8, 2098, 7, 1190, 7, 16103, 10, 11, 450, 34, 19, 81, 10, 18, 105, 446, 133, 369, 120, 13, 40, 2950, 9, 7, 1238, 10, 63, 8, 524, 1192, 19, 772, 13, 4677, 4159, 10, 146, 141, 168, 8, 1367, 11, 4864, 7, 8473, 7, 20241, 22, 21, 7, 0, 21, 55, 7, 10613, 7, 0, 22, 21, 7, 58605, 7894, 7, 0, 7, 0, 21, 20, 15, 450, 34, 8, 136, 13, 8, 187, 572, 9, 7, 77, 1975, 15, 670, 9, 24, 7, 435, 36, 7, 3306, 33, 96, 21, 7, 24470, 529, 7, 0, 7, 52626, 21, 36, 21, 7, 24470, 529, 7, 229, 7, 5188, 21, 33, 3], [2, 7, 19, 31, 15, 31096, 363, 27, 21, 7, 48165, 0, 3145, 7, 23004, 92, 6915, 0, 7, 0, 21, 11, 216, 125, 583, 12, 188, 14, 42, 4429, 17, 8, 116, 13, 80, 288, 9634, 9, 7, 106, 10, 216, 125, 38, 70, 472, 189, 1932, 254, 16, 268, 14, 84, 628, 64, 22, 1422, 9, 7, 346, 10, 1068, 31, 60, 2256, 11, 32, 38, 882, 14, 126, 12, 2359, 31, 189, 173, 21, 7, 10014, 7, 52627, 4468, 7, 52628, 21, 91, 328, 14, 41, 12, 146, 6567, 9, 7, 1299, 10, 18, 236, 2683, 2256, 30, 28, 8, 700, 23, 13592, 61, 54, 19, 15, 35, 12, 464, 9, 7, 151, 288, 710, 10, 215, 10, 38, 37, 8, 2052, 13, 8, 232, 31, 1188, 189, 57, 8, 7317, 5178, 128, 8170, 670, 2733, 330, 9, 24, 7, 26, 28, 8, 31, 10, 16, 22, 59, 8, 5145, 13, 7, 1700, 7, 5218, 9, 7, 19, 15, 12, 694, 464, 10, 26, 7, 1064, 7, 2498, 308, 59, 26, 94, 53, 7, 5218, 26, 7, 4174, 7, 5955, 9, 7, 17, 74, 116, 1685, 91, 39, 184, 53, 7, 5218, 9, 7, 39, 22, 1019, 8, 6823, 10, 60, 8, 378, 1107, 1430, 11, 418, 11, 15, 57, 37, 76, 517, 17, 120, 116, 36, 1729, 89, 34, 19, 10, 18, 258, 48, 7, 300, 7, 498, 1536, 11, 90, 38, 1500, 14, 140, 151, 456, 13, 200, 50, 33, 9, 7, 8, 164, 136, 7, 5218, 25, 12, 7, 2903, 7, 325, 786, 11, 591, 53, 8, 462, 34, 8, 7, 1289, 7, 972, 9901, 2310, 8812, 9, 7, 899, 10, 798, 65, 102, 305, 14, 1644, 8, 579, 28, 7, 2498, 17, 8, 3512, 15, 57, 2333, 9, 7, 133, 181, 55, 253, 60, 7, 1064, 7, 2498, 2826, 204, 52, 5189, 50, 50, 7, 39, 25, 12, 500, 305, 92, 30, 446, 37, 12, 1133, 786, 55, 5189, 1700, 9, 24, 7, 17, 1643, 14, 8, 394, 1069, 10, 7, 1700, 7, 5218, 22, 340, 25, 17, 74, 116, 53, 19, 31, 9, 7, 16, 22, 595, 20, 8, 31, 1188, 38, 177, 18557, 17, 34, 8, 944, 12769, 59, 13364, 3894, 8, 340, 13, 10073, 10, 37, 7, 5218, 9, 7, 5218, 25, 341, 17, 7, 2665, 10, 8946, 36, 37, 7, 6699, 33, 47, 12, 3998, 12770, 27, 4875, 1926, 710, 189, 37, 12, 537, 13, 347, 27, 11442, 9, 7, 215, 10, 1960, 14, 110, 7150, 10, 64, 177, 527, 7, 5218, 36, 143, 127, 1813, 330, 33, 86, 4781, 4546, 189, 49, 31097, 11, 31097, 11, 31097, 14, 8366, 12, 3863, 36, 14, 74, 10377, 33, 11, 133, 2504, 6731, 80, 933, 55, 21749, 17, 8, 1648, 9, 7, 17, 102, 691, 10, 53, 7, 733, 7, 2665, 36, 49, 25, 677, 527, 47, 2706, 20548, 69, 2086, 27, 44737, 33, 39, 1092, 711, 14, 34669, 9, 7, 17, 8, 29, 46, 208, 178, 226, 1685, 92, 102, 93, 863, 7, 1700, 7, 5218, 25, 341, 9, 24, 7, 107, 8, 31, 3289, 241, 74, 7727, 14, 164, 498, 10, 16, 22, 53, 12, 498, 2160, 26, 4152, 51, 307, 51, 175, 1371, 55, 307, 27, 12, 4875, 1187, 6801, 9, 7, 153, 37, 104, 1491, 7006, 10, 992, 4017, 11, 8, 7, 2999, 6177, 154, 32, 198, 45, 16, 66, 50, 66, 50, 7, 1193, 51, 65, 543, 137, 11, 388, 1207, 10, 107, 8, 246, 15, 2155, 5986, 627, 10, 18, 58, 35, 408, 272, 126, 16, 9, 7, 16, 22, 57, 12, 621, 11, 2155, 940, 9, 3], [2, 18, 82, 955, 34, 83, 70, 67, 125, 11, 34, 2155, 1921, 9, 7, 77, 5282, 15, 14, 362, 100, 49, 203, 14, 84, 103, 125, 14, 1141, 80, 75, 23, 11, 304, 23, 6752, 9, 24, 18, 104, 203, 14, 550, 100, 3070, 80, 75, 34, 1229, 10, 11, 203, 14, 39743, 8, 211, 20, 8, 170, 124, 1257, 13, 151, 1229, 125, 197, 35, 98, 262, 27, 16, 28, 70, 218, 9, 7, 90, 105, 186, 61, 49, 32, 38, 11, 105, 1975, 27, 61, 2037, 23, 11, 34670, 9, 24, 7, 19, 31, 707, 750, 101, 8, 1229, 2602, 9, 24, 7, 8, 170, 11, 572, 15, 7, 320, 7, 29196, 9, 7, 16, 22, 236, 12, 97, 1945, 69, 8, 572, 15, 104, 8, 170, 9, 7, 294, 39, 509, 127, 959, 32174, 9, 7, 39, 155, 35, 98, 120, 9, 7, 52, 420, 8, 413, 23, 7, 320, 29196, 9, 7, 11, 63, 32, 84, 255, 345, 47, 108, 10, 837, 16, 9, 24, 18, 491, 35, 157, 255, 59, 8, 130, 23, 415, 41, 494, 9, 18, 258, 12, 138, 3690, 47, 109, 94, 8, 170, 1192, 14, 9400, 17, 14, 8, 356, 256, 22, 410, 69, 71, 15, 2471, 11, 2161, 9, 7, 151, 218, 7128, 685, 38, 12, 138, 9524, 11, 228, 157, 158, 59, 8, 1071, 13, 350, 13, 7, 607, 7, 29196, 9, 7, 294, 39, 155, 98, 9352, 362, 9, 24, 7, 213, 494, 9, 7, 16, 22, 627, 23, 58, 35, 473, 147, 75, 34, 16, 9, 3], [2, 7, 69, 32, 184, 45, 8, 1011, 11, 369, 533, 59, 16, 48, 1097, 285, 554, 13, 29, 293, 14, 350, 93, 64, 32, 98, 149, 9, 7, 115, 193, 294, 18, 369, 8, 2681, 28, 8, 102, 29, 460, 21, 7, 0, 21, 321, 26, 54, 86, 127, 118, 13, 19, 435, 646, 59, 8, 187, 75, 27, 216, 1949, 1634, 20, 85, 1337, 779, 17, 1114, 9, 7, 215, 10, 898, 533, 59, 20, 29, 149, 18, 140, 18, 234, 19, 43, 11, 37, 20, 43, 11, 20, 29, 15, 76, 358, 64, 43, 73, 842, 12, 29, 27, 20, 435, 73, 42, 59, 9, 18, 105, 42, 1155, 10, 18, 531, 68, 13, 12, 1059, 554, 453, 11, 32, 98, 20, 17, 19, 29, 14, 65, 2580, 9, 7, 215, 10, 54, 15, 68, 533, 1176, 8, 8367, 11, 673, 1720, 26, 8, 648, 150, 13, 8, 100, 128, 593, 262, 47, 8, 14784, 45, 8, 478, 13, 8, 31, 105, 18808, 14, 9, 7, 8, 29, 104, 60, 8, 176, 772, 13, 379, 182, 1256, 46, 58, 37, 4125, 14, 41, 65, 944, 985, 10, 17, 19, 438, 16, 15, 17, 211, 12, 16306, 9, 7, 8, 102, 29, 18, 58, 37, 122, 83, 60, 20, 1337, 813, 414, 20, 5158, 231, 17, 8, 29, 11, 18, 84, 8, 590, 28, 19, 43, 15, 1998, 117, 10, 152, 16, 25, 57, 37, 8, 29, 18, 25, 1042, 9, 3], [2, 7, 2844, 10614, 36, 27, 14928, 21, 4949, 21, 224, 0, 15238, 2426, 10, 21, 7, 2111, 21, 3728, 12, 188, 13, 4735, 14, 872, 163, 11, 105, 261, 492, 141, 110, 803, 132, 30, 8, 425, 2899, 2651, 314, 11, 7, 948, 7, 18037, 10, 49, 10255, 40, 674, 17, 241, 190, 150, 10, 104, 426, 48, 1296, 1133, 260, 9, 12, 17154, 10, 14, 42, 273, 10, 30, 8, 68, 5248, 21, 7, 1217, 21, 10, 113, 127, 174, 330, 10, 15, 12, 3832, 1505, 960, 9, 18, 1487, 32, 126, 20, 321, 9, 36, 194, 383, 124, 263, 33, 3], [2, 7, 69, 7, 360, 15, 281, 14, 4598, 64, 48, 21, 1108, 419, 21, 15, 53, 10, 46, 1968, 52, 3242, 10, 1408, 16, 268, 1507, 691, 17, 8, 1688, 13, 8, 17591, 21, 1175, 21, 9, 24, 7, 226, 10, 120, 1175, 2327, 207, 281, 14, 11133, 17, 40, 13107, 45, 190, 1965, 9, 7, 273, 10, 46, 2827, 61, 17, 5348, 11, 1213, 1759, 529, 16, 22, 37, 53, 46, 38, 36, 48166, 4865, 33, 10256, 8901, 49, 133, 41, 12, 11839, 9, 24, 7, 899, 10, 63, 32, 38, 12, 1175, 32, 140, 44, 59, 7, 10257, 11, 7, 498, 11, 7, 2533, 11, 13, 286, 32, 198, 1360, 72, 14, 1327, 27, 1970, 714, 11, 12, 11280, 5601, 13, 111, 9, 7, 17155, 151, 200, 10, 53, 10, 44, 159, 311, 2883, 533, 10, 24471, 66, 24, 7, 899, 10, 32, 1244, 207, 27, 12, 12156, 44, 8, 75, 9, 7, 32, 38, 57, 12, 7380, 482, 13, 12, 32, 23, 140, 23, 64, 10, 20, 22, 109, 16, 15, 10, 26889, 44, 9, 24, 7, 11, 13, 286, 32, 3456, 10, 53, 307, 49, 133, 15930, 181, 10, 30, 32, 3456, 17155, 16, 22, 53, 597, 2883, 533, 10, 24471, 9, 7, 11, 32, 198, 285, 9, 7, 20, 15, 2758, 9, 24, 7, 11, 13, 286, 32, 78, 551, 529, 32, 198, 12, 5602, 9, 12, 5602, 49, 676, 75, 14, 2038, 52629, 1221, 1937, 39, 91, 35, 4963, 23477, 9, 7, 11, 1937, 39, 91, 35, 3456, 55, 2716, 3230, 107, 39, 1111, 12, 7099, 3370, 36165, 9, 24, 7, 11, 32, 1327, 12, 25624, 1109, 1351, 529, 7, 11506, 7, 2073, 9, 7, 88, 10, 18, 491, 35, 76, 955, 7, 2201, 7, 7266, 9, 7, 731, 7, 796, 60, 617, 8, 4258, 34, 8, 428, 494, 9, 24, 7, 19, 29, 15, 12, 21750, 13, 12, 7, 18038, 205, 7, 21751, 554, 1351, 36, 17, 102, 691, 6407, 1390, 13, 111, 33, 96, 21, 7, 1150, 10, 20, 22, 64, 18, 73, 42, 53, 63, 18, 25, 12, 1175, 9, 21, 7, 30, 394, 100, 11, 394, 7517, 17, 19, 438, 78, 37, 842, 8, 477, 13, 9635, 9, 3], [2, 7, 4686, 865, 8727, 7, 26236, 7, 34671, 15, 9164, 61, 13, 289, 27, 19, 5603, 702, 269, 1613, 11, 6274, 81, 20, 2531, 51, 12, 603, 618, 13, 937, 6864, 9, 7, 34671, 209, 14, 41, 58606, 9963, 75, 11, 7188, 28, 8, 106, 214, 26, 820, 7, 948, 7, 9165, 11, 7, 18809, 7, 9401, 298, 138, 800, 101, 80, 587, 10, 4006, 434, 19936, 11, 229, 1742, 9, 24, 7, 3557, 7895, 7151, 7, 6016, 21, 7, 11443, 21, 7, 23968, 538, 26, 48, 974, 145, 34, 12, 2032, 316, 20, 287, 97, 11, 240, 40, 335, 527, 17, 8, 1648, 9, 7, 39, 3024, 51, 1096, 11, 1240, 715, 61, 14, 513, 8, 335, 13, 8, 1216, 49, 527, 40, 9, 7, 3280, 13, 1613, 38, 58607, 14, 407, 108, 51, 404, 14, 8, 364, 13, 8, 1419, 49, 60, 99, 1664, 14, 175, 1550, 30, 73, 35, 32, 140, 17, 8, 125, 486, 403, 90, 41, 7, 11443, 21752, 2037, 539, 8, 1378, 36, 49, 217, 824, 20, 654, 12, 2603, 162, 14, 8, 351, 25, 12, 497, 829, 33, 154, 12, 1066, 13, 1613, 7754, 11, 20914, 143, 64, 212, 14, 214, 9, 7, 497, 5956, 66, 7, 32, 155, 84, 16, 9, 7, 16, 22, 44, 13, 20, 11, 68, 9, 24, 7, 19937, 7, 19349, 22, 380, 179, 91, 12, 543, 316, 13, 2183, 1493, 14, 8, 12450, 30, 8, 817, 15, 26890, 11, 29197, 11, 16, 23969, 8, 31, 13, 112, 780, 11, 1082, 9, 7, 26, 7, 23968, 10, 7, 17592, 7, 6390, 15, 8, 139, 169, 17, 8, 31, 8401, 14, 7755, 103, 2430, 26, 39, 7862, 51, 3772, 1317, 14, 2564, 9, 7, 151, 9791, 1193, 7, 488, 6154, 1041, 17, 4727, 11, 2431, 254, 112, 759, 741, 9, 7, 159, 7, 865, 176, 7, 26236, 9, 3], [2, 7, 270, 29, 144, 131, 9, 7, 270, 137, 117, 9, 18, 78, 37, 842, 12, 29, 463, 115, 19, 9, 7, 178, 14, 84, 9, 7, 74, 137, 45, 44, 9, 2367, 1339, 36, 171, 33, 155, 184, 28, 175, 316, 9, 18, 58608, 409, 49, 25, 394, 213, 14, 177, 298, 304, 101, 19, 29, 9, 24, 18, 167, 770, 28, 7, 2122, 7, 2652, 9, 7, 225, 42, 1191, 92, 18, 78, 37, 842, 109, 0, 40, 19350, 225, 42, 14, 4318, 654, 8, 316, 50, 24, 7, 8, 2133, 17, 8, 29, 92, 400, 46, 146, 1334, 14, 11840, 9, 24, 7, 26, 28, 8, 997, 145, 92, 64, 12, 471, 50, 7, 39, 146, 42, 298, 34, 12, 471, 11, 783, 54, 50, 18, 78, 84, 108, 128, 68, 1148, 45, 12654, 271, 93, 137, 9, 24, 7, 455, 709, 96, 7, 58, 37, 821, 92, 58, 37, 810, 50, 3]...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb\n",
       "y: ItemList (90084 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb\n",
       "\n",
       "Valid: LabeledData\n",
       "x: TextList (9916 items)\n",
       "[[2, 7, 49406, 2468, 66, 50, 18, 167, 770, 30, 76, 28, 8, 1031, 22, 20, 22, 57, 116, 117, 906, 14, 42, 2588, 671, 92, 7, 32, 78, 1410, 8, 232, 8, 1060, 10767, 584, 107, 16, 25, 15338, 176, 118, 11, 264, 296, 10, 30, 20, 22, 99, 245, 301, 10, 52, 8, 703, 934, 14, 42, 12, 138, 358, 1242, 10, 37, 76, 68, 8926, 50, 7, 899, 10, 8, 106, 29, 85, 8, 1273, 13, 7, 1459, 7, 5707, 23, 12, 145, 49, 95, 76, 114, 6374, 0, 17, 4820, 0, 328, 7747, 50, 23, 20, 19, 43, 6665, 1495, 10, 52, 54, 25, 74, 62, 2400, 62, 17, 255, 20, 581, 10, 16, 57, 481, 672, 9, 24, 7, 3425, 18, 82, 234, 19, 301, 69, 18, 25, 712, 10, 30, 47, 115, 494, 128, 12, 678, 357, 13, 8, 232, 18, 420, 128, 5408, 9, 7, 14, 89, 10, 54, 15, 74, 703, 14, 7, 5915, 7, 329, 10, 57, 12, 5085, 2649, 20, 91, 35, 1802, 120, 25929, 1685, 9, 3], [2, 12, 3372, 13, 13252, 210, 1109, 6923, 2299, 1164, 14, 159, 8157, 27, 12, 2142, 9, 12, 1336, 29357, 203, 14, 3562, 111, 14, 3759, 8, 4149, 233, 8, 342, 2884, 10, 46, 104, 41, 14, 11482, 27, 8171, 10, 48, 7, 1323, 11, 12, 11676, 6459, 778, 554, 169, 9, 7, 295, 168, 28, 82, 382, 999, 10, 7, 733, 62, 7, 3121, 62, 7, 6062, 36, 12, 1110, 13713, 629, 23, 29, 7776, 33, 15, 34, 521, 26, 12, 16117, 11, 8, 102, 263, 4125, 14, 8, 1481, 7, 15454, 36, 17, 43, 13, 82, 382, 700, 23, 1462, 587, 71, 85, 33, 9, 7, 216, 41, 70, 416, 587, 9, 7, 117, 97, 299, 345, 17, 8, 29, 15, 13873, 97, 9, 24, 7, 77, 7, 1232, 96, 11216, 24, 7, 34835, 288, 7, 2353, 96, 7, 232, 7, 1597, 24, 7, 765, 7, 1866, 96, 222, 14722, 13, 3561, 10, 263, 11368, 3], [2, 18, 1576, 19, 548, 16, 73, 42, 206, 67, 57, 47, 8, 1011, 13, 8, 29, 438, 9, 7, 1646, 11, 7, 6382, 684, 61, 206, 67, 488, 3162, 8, 145, 49, 527, 40, 335, 34, 12, 6329, 27, 12, 597, 917, 10, 30, 19, 29, 208, 11167, 13996, 26, 16, 437, 34, 9, 7, 604, 7, 4490, 15, 1174, 305, 282, 69, 39, 310, 12, 231, 53, 19, 117, 97, 8, 29, 25, 12, 449, 13, 627, 16, 83, 1021, 40, 641, 9, 7, 1646, 11, 7, 6382, 25, 88, 1054, 998, 18, 542, 16, 12, 382, 155, 41, 542, 16, 12, 263, 10, 18, 542, 16, 48, 1538, 337, 57, 107, 7, 604, 7, 4490, 22, 917, 25, 597, 9, 3], [2, 7, 63, 16, 86, 35, 28, 8, 817, 61, 13, 2692, 691, 11, 12, 17699, 13407, 69, 43, 123, 220, 175, 8, 3849, 10, 16, 73, 42, 753, 14, 1443, 19, 377, 23, 354, 21174, 28, 12, 7, 847, 23, 7, 846, 1197, 1727, 9, 7, 8, 130, 59, 8, 1254, 22, 1057, 14, 2286, 12, 537, 13, 0, 16, 1933, 4085, 26, 1293, 2480, 657, 28643, 213, 27, 8, 573, 10, 332, 4813, 10, 17, 4711, 13, 12, 17369, 4768, 5801, 34, 1151, 9, 7, 3006, 10, 8, 29, 15, 7942, 10, 368, 10, 742, 11, 76, 3919, 13, 237, 326, 309, 9, 7, 367, 23, 5151, 7, 2379, 7, 11631, 10, 7, 2264, 60, 12, 1291, 231, 26, 8, 479, 1254, 22932, 49, 13668, 8, 2258, 11, 166, 509, 14, 9819, 44, 9568, 9, 7, 39, 2327, 163, 8, 192, 11, 16, 15, 268, 14, 409, 153, 39, 2504, 9, 7, 102, 1083, 13, 8, 196, 58, 12, 543, 316, 27, 12, 246, 20, 3798, 138, 9, 3], [2, 7, 109, 15, 16, 637, 14, 114, 161, 12, 97, 29, 27, 161, 171, 66, 7, 86, 46, 882, 101, 16, 66, 7, 8, 130, 60, 178, 14, 58, 27, 48, 339, 13, 109, 200, 73, 492, 61, 63, 12, 3131, 2140, 28, 1700, 9, 7, 46, 58, 35, 76, 372, 14, 220, 48, 1468, 13, 20, 9, 7, 57, 69, 32, 217, 32, 86, 168, 12, 955, 51, 835, 13276, 34, 8946, 2533, 36, 8, 106, 757, 248, 33, 10, 8, 29, 1146, 141, 8, 1178, 11, 101, 629, 23, 31, 465, 59, 383, 33, 12, 1294, 8368, 6152, 10, 263, 33, 8, 2011, 479, 4495, 3245, 49, 509, 14, 1011, 16, 72, 27, 8, 110, 4027, 434, 17, 498, 10, 11, 382, 33, 12, 5527, 30, 578, 206, 645, 16243, 49, 519, 14, 402, 8, 574, 1700, 59, 19, 9, 7, 71, 22, 544, 8, 1378, 13, 8, 479, 1682, 23, 4969, 1168, 23, 10176, 13, 8, 1294, 1066, 10, 49, 23, 321, 13, 825, 56, 23, 1165, 56, 14, 114, 56, 328, 26593, 9, 7, 30, 10, 69, 71, 240, 14, 8946, 10, 71, 91, 35, 402, 108, 9, 7, 17, 211, 10, 8, 29, 115, 1318, 51, 629, 23, 31, 465, 10, 14, 3045, 629, 23, 31, 23, 134, 23, 465, 9, 7, 72, 14, 166, 10, 90, 38, 52, 247, 141, 8, 232, 1875, 242, 13, 8, 29, 10, 20, 110, 100, 492, 16, 141, 9, 18, 241, 87, 9, 7, 63, 16, 57, 95, 160, 99, 1108, 134, 465, 10, 30, 74, 50, 7, 16, 22, 37, 50, 7, 16, 22, 8, 265, 13, 21, 10228, 18, 167, 52, 4207, 18, 167, 128, 394, 44, 8, 75, 10, 52, 615, 134, 89, 28, 0, 13, 134, 465, 9, 7, 44, 27, 12, 582, 10, 582, 1082, 10, 20, 60, 178, 14, 58, 27, 370, 8, 1016, 130, 13, 8, 29, 10, 8, 1682, 23, 4969, 130, 13, 8, 29, 10, 55, 8, 223, 130, 13, 8, 29, 9, 7, 44, 1634, 1968, 34, 44, 2053, 10, 79, 190, 640, 244, 13, 3748, 735, 1863, 32, 13, 9, 7, 8, 134, 192, 60, 14, 42, 8, 939, 13, 7070, 315, 12, 2545, 17680, 21, 1339, 10, 54, 60, 14, 42, 12, 4064, 13, 134, 224, 8, 1700, 6544, 11, 8, 578, 206, 645, 16243, 10, 1150, 10, 20, 257, 179, 50, 7, 1334, 16, 17, 54, 50, 21, 24, 7, 1907, 10, 7, 1701, 7, 333, 15, 28045, 11, 298, 101, 12, 231, 135, 39, 91, 35, 233, 72, 27, 43, 689, 7, 1701, 7, 333, 366, 9, 7, 8, 7, 1701, 7, 333, 2682, 15, 3193, 47, 12, 442, 28, 16, 14, 2295, 4984, 17, 8, 31, 10, 79, 16, 133, 91, 9, 7, 11, 7, 1557, 7, 4107, 15, 1333, 101, 12, 1514, 27, 507, 1085, 10, 14, 13151, 8, 952, 927, 13, 8, 507, 1304, 13, 8, 7, 1700, 22, 4761, 4062, 17, 7, 0, 9, 7, 168, 7, 1557, 7, 4107, 128, 28045, 53, 7, 1701, 7, 333, 17, 587, 20, 1360, 3066, 20744, 14, 497, 4820, 11, 7, 0, 23, 6266, 10, 30, 1968, 53, 7, 4221, 190, 75, 10, 15, 53, 168, 12, 103, 1181, 6070, 1645, 34, 12, 1951, 9, 7, 4864, 9, 24, 7, 11, 115, 10, 2391, 8, 377, 242, 13, 8, 238, 29, 96, 7, 16, 4925, 8, 0, 10, 9173, 890, 11010, 901, 96, 7, 155, 7, 1535, 7, 1570, 159, 34, 14, 42, 1700, 10, 1279, 20, 39, 208, 8960, 107, 13, 12, 1294, 19432, 66, 7, 8, 7, 300, 7, 1010, 7, 11, 7, 44, 7, 67, 17249, 26, 39, 522, 201, 8, 20069, 34, 440, 264, 10, 53, 7, 13222, 8, 809, 73, 9, 7, 27, 8, 681, 7, 6502, 7, 15059, 7, 13, 7, 8, 7, 20069, 11, 8, 24493, 23, 52, 23, 1252, 23, 20, 23, 24493, 23, 254, 23, 498, 23, 875, 20, 7, 0, 23602, 17, 6757, 8641, 28, 8, 381, 1474, 174, 9, 24, 7, 8, 170, 11, 277, 572, 10, 7, 2830, 7, 13305, 10, 15, 166, 34, 77, 908, 1008, 13, 941, 11, 988, 18, 167, 4035, 262, 51, 1446, 9, 7, 19, 31, 225, 42, 131, 26, 12, 28405, 13, 12, 14435, 388, 1648, 10, 135, 100, 36, 0, 7, 2830, 7, 13305, 33, 208, 14, 1141, 388, 304, 711, 14, 80, 908, 1489, 10, 11, 37, 80, 1922, 9, 7, 19, 15, 12, 1120, 113, 14, 4898, 862, 6875, 100, 22, 665, 28, 625, 1024, 10, 11, 8, 10335, 1080, 103, 171, 86, 9433, 101, 11894, 17, 16, 9, 7, 20, 22, 8, 82, 1810, 54, 78, 42, 9, 24, 5874, 96, 7, 8, 8368, 6152, 17, 8, 5667, 25, 711, 14, 8, 1383, 4874, 17, 7, 19486, 10, 7, 0, 11, 7, 7880, 9, 7, 13, 286, 16, 25, 10, 64, 345, 95, 16, 42, 69, 32, 942, 12, 246, 11, 197, 35, 402, 12, 1294, 51, 12, 8690, 23, 9428, 1586, 9, 3], [2, 7, 8, 434, 17, 8, 435, 13, 19, 744, 38, 8, 106, 434, 17, 19, 31, 22, 772, 635, 10, 12, 1874, 5222, 1942, 13, 8, 36, 17, 77, 696, 504, 33, 635, 21, 7, 77, 541, 200, 21, 51, 21, 7, 8, 7, 497, 13, 7, 239, 21, 9, 7, 11, 19, 266, 138, 1522, 15, 35, 8, 82, 1278, 20, 180, 21, 7, 8, 7, 674, 7, 2121, 21, 2574, 9, 21, 7, 924, 7, 623, 21, 36, 5514, 33, 16526, 14, 77, 1656, 7, 623, 124, 7, 1217, 470, 9, 7, 8, 31, 10, 79, 25, 1034, 241, 1097, 47, 53463, 7, 11319, 7, 2264, 10, 49, 2708, 26, 1257, 10, 572, 10, 170, 11, 997, 145, 26, 8, 11711, 7, 1015, 7, 623, 10, 15, 627, 10, 74, 808, 10, 30, 16, 15, 104, 683, 808, 20, 16, 15, 1129, 10, 11, 20, 312, 586, 10, 261, 7, 11319, 282, 10, 25, 1861, 20, 46, 86, 37, 628, 254, 12, 958, 9, 24, 7, 1015, 7, 8280, 36, 7, 11319, 33, 12, 835, 30, 483, 2030, 3015, 7005, 10, 12095, 40, 2986, 335, 7, 0, 10, 12, 2082, 10, 17, 48, 1637, 9, 7, 363, 27, 40, 45042, 2848, 7, 5996, 36, 7, 3118, 7, 0, 33, 10, 39, 25222, 8443, 338, 210, 365, 17, 649, 14, 1718, 332, 12, 183, 10, 430, 335, 61, 13, 80, 674, 23, 530, 92, 24, 21, 7, 924, 7, 623, 21, 15, 24627, 12, 31, 13, 8, 62, 52, 97, 16, 22, 67, 265, 62, 10, 30, 16, 15, 104, 60, 2366, 683, 8, 681, 662, 38012, 9, 7, 1179, 1078, 60, 236, 99, 43, 13, 77, 1543, 541, 7, 202, 5443, 10, 11, 10, 26, 12, 520, 13, 211, 10, 16, 15, 104, 43, 13, 8, 8050, 5443, 28, 662, 7, 623, 7, 1217, 1429, 9, 7, 549, 341, 34, 12, 3774, 354, 10, 21, 7, 924, 7, 623, 21, 4269, 65, 3101, 14, 8, 21, 7, 2966, 21, 125, 10, 282, 7, 626, 7, 6070, 22, 958, 21, 7, 3137, 13, 7, 2966, 21, 36, 8754, 33, 10, 11, 3641, 8, 184, 13, 8, 436, 7, 4434, 124, 7, 20290, 7, 2202, 7, 1701, 7, 623, 1429, 161, 26, 21, 7, 557, 7, 4779, 21, 36, 9047, 33, 23, 82, 20, 19, 308, 12, 188, 8374, 11, 32274, 9, 7, 549, 53463, 7, 11319, 22, 5912, 25, 37, 1533, 14, 114, 12, 266, 623, 508, 96, 7, 128, 12, 271, 1526, 10, 962, 23, 279, 1656, 10, 40, 231, 13, 7, 1015, 7, 8280, 542, 7, 11319, 8, 1421, 14, 114, 61, 27, 12, 384, 13, 904, 10, 12213, 2010, 210, 365, 36, 49, 73, 330, 148, 72, 26, 674, 23, 192, 35252, 17, 7, 1015, 7, 8280, 22, 8084, 33, 9, 24, 7, 110, 13, 8, 623, 15, 177, 206, 88, 23, 113, 2666, 8, 549, 2245, 354, 9, 7, 8, 425, 1764, 65, 578, 611, 434, 36, 21, 7, 98, 20, 10, 16, 251, 42, 8, 1220, 92, 11, 298, 12, 6204, 34, 52, 46, 58, 35, 84, 32, 198, 12, 14182, 9, 21, 33, 9, 7, 1392, 8, 3837, 772, 635, 10, 21, 7, 924, 7, 623, 21, 104, 1764, 12, 1874, 2026, 1263, 47, 12, 664, 1235, 460, 62, 7, 966, 7, 9414, 11, 8, 7, 28992, 62, 23, 77, 183, 541, 1235, 10, 37, 9, 7, 28, 8, 398, 13, 8, 31, 10, 18, 794, 1573, 768, 7, 966, 7, 9414, 11, 7, 3118, 7, 0, 10, 49, 310, 8, 45042, 2848, 10, 38, 3566, 55, 76, 8, 187, 419, 23, 8, 127, 184, 628, 8, 187, 10, 11, 280, 127, 6488, 13058, 10, 806, 23, 11480, 462, 279, 19, 962, 17, 43, 31, 73, 42, 12, 678, 5255, 9, 7, 102, 93, 53463, 7, 11319, 10, 110, 13, 8, 196, 1083, 133, 87, 120, 102, 125, 9, 7, 19, 15, 8, 106, 31, 18, 160, 131, 61, 13, 8, 191, 47, 7, 11319, 9, 7, 1110, 10, 8, 145, 1092, 13, 4376, 17, 6728, 9, 24, 7, 455, 10, 21, 7, 924, 7, 623, 21, 15, 12, 31, 20, 446, 15, 35, 28, 312, 9, 7, 26, 12, 520, 13, 211, 10, 16, 15, 945, 627, 9, 7, 30, 16, 15, 104, 1129, 10, 11, 16526, 14, 77, 1656, 470, 13, 8715, 11, 687, 1242, 533, 9, 7, 51352, 155, 142, 12, 0, 51, 19, 31, 469, 62, 1217, 508, 62, 9, 3], [2, 7, 5317, 73, 13, 99, 24506, 10, 63, 39, 698, 20, 40, 958, 25, 128, 914, 61, 4973, 50, 50, 50, 7, 104, 8, 171, 49, 276, 8, 4973, 38, 2126, 5768, 9, 7, 711, 14, 8, 211, 20, 414, 12, 3560, 15, 37, 137, 16, 15, 57, 677, 421, 178, 9, 7, 74, 43, 83, 105, 476, 49, 8, 3560, 25, 9, 100, 82, 476, 49, 276, 12, 694, 231, 17, 53, 48, 869, 29, 9, 19, 15, 57, 640, 109, 32, 95, 6276, 161, 48, 523, 145, 11, 40, 2678, 10, 47, 19, 998, 138, 29, 9, 7, 19, 155, 42, 12, 782, 11, 4, 222, 9, 18, 197, 1153, 283, 32, 73, 2767, 12, 306, 53, 20, 9, 18, 217, 8, 29, 25, 452, 662, 11, 155, 42, 2442, 4, 222, 50, 7, 16, 483, 4284, 64, 147, 1352, 14, 42, 404, 51, 898, 16, 9, 7, 147, 57, 254, 16, 12, 219, 1006, 9, 3], [2, 7, 219, 35387, 519, 14, 407, 2483, 224, 8, 13601, 11, 80, 806, 972, 9, 24, 7, 8043, 36, 48, 3799, 7, 1223, 7, 6919, 33, 60, 14, 372, 11, 1882, 14, 8, 722, 7, 1323, 2416, 36, 7, 5931, 7, 3771, 10, 7, 2264, 10, 3284, 14, 42, 1924, 10, 26, 681, 33, 20, 40, 482, 25, 527, 47, 8, 4539, 22, 997, 3189, 49, 60, 104, 99, 1973, 17, 6633, 23, 16322, 69, 1258, 22, 279, 9, 7, 53568, 7, 210, 3347, 10, 173, 39, 1744, 40, 6633, 53, 16, 22, 12, 24837, 9, 24, 600, 865, 23, 1143, 540, 47, 127, 462, 10, 1068, 13, 932, 60, 129, 923, 34, 40, 6695, 10, 30, 19, 508, 2362, 35, 117, 97, 11, 16, 60, 12, 359, 361, 636, 75, 13, 15998, 248, 9, 3], [2, 7, 17, 8, 4446, 4036, 13, 21, 7, 336, 7, 539, 21, 8, 1188, 87, 37, 476, 14, 298, 120, 265, 13, 2611, 101, 8, 130, 9, 7, 284, 44, 2480, 34, 8, 1371, 41, 99, 28916, 47, 8, 7, 2237, 7, 6063, 10, 7, 195, 7, 325, 3593, 1088, 785, 45, 8, 9626, 13, 12, 3849, 9, 7, 745, 12, 191, 3396, 23, 1628, 8, 1218, 23, 1404, 13, 43, 13, 8, 302, 3120, 1135, 51, 13529, 11472, 108, 14, 6599, 3881, 14, 12, 7687, 7466, 13, 134, 229, 120, 6454, 13, 475, 9, 24, 7, 30, 7391, 19, 31, 15, 83, 48, 3182, 23, 870, 3156, 9, 7, 119, 8, 1700, 13, 8, 7, 2237, 7, 1569, 6128, 7, 2469, 26, 40, 11476, 39, 1240, 496, 12, 1689, 11941, 11, 3250, 72, 12, 15510, 17, 8, 658, 13, 8, 512, 9, 7, 12165, 7, 4103, 7, 6626, 105, 42, 70, 3935, 69, 39, 1136, 19, 31, 50, 3], [2, 18, 140, 77, 2681, 979, 70, 2616, 10, 30, 19, 31, 60, 70, 1746, 1384, 9, 7, 8, 845, 7, 948, 61, 54, 73, 41, 12, 268, 75, 5645, 27, 19, 31, 9, 7, 8, 458, 31, 3247, 13, 1306, 0, 421, 80, 2832, 11, 1243, 34, 4812, 24827, 9, 7, 19, 15, 44, 245, 27, 70, 25118, 11, 5895, 2772, 11, 8, 31, 251, 42, 103, 14, 142, 14, 11791, 17, 48, 552, 4915, 9, 7, 215, 10, 930, 32, 83, 134, 19, 456, 13, 552, 55, 38, 12, 7, 2414, 49, 1321, 125, 59, 147, 2065, 1310, 10, 115, 19, 15, 261, 182, 14, 42, 381, 14, 1180, 14, 1437, 9, 18, 41, 12, 271, 322, 7203, 28, 19, 456, 13, 169, 11, 76, 18, 85, 14, 1067, 563, 14, 126, 119, 12, 384, 248, 9, 18, 78, 1173, 8, 179, 20, 437, 101, 16, 10, 30, 16, 22, 57, 37, 1555, 9, 3]...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb\n",
       "y: ItemList (9916 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll # LabeledList item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj alan xxmaj rickman & xxmaj emma xxmaj thompson give good performances with southern / xxmaj new xxmaj orleans accents in this detective flick . xxmaj it 's worth seeing for their scenes- and xxmaj rickman 's scene with xxmaj hal xxmaj holbrook . xxmaj these three actors xxunk to entertain us no matter what the movie , it seems . xxmaj the plot for the movie shows potential , but one gets the impression in watching the film that it was not pulled off as well as it could have been . xxmaj the fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things . xxmaj the movie is worth a xxunk if for nothing more than entertaining performances by xxmaj rickman , xxmaj thompson , and xxmaj holbrook . xxeos\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x_obj(0) # access the underlying texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ll, open(data_dir/'ld.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(data_dir/'ld.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Batches for LM tasks (predict next word) and wrapping everything into databunch\n",
    "### Making (x,y) pairs where y is the next word of x \n",
    "1. x.shape = y.shape = (bs, bptt) \n",
    "2. `bs` vs. `seq_len` vs `bptt` \n",
    "    - `bs` is the number of docs/sentences we are working on at the same time\n",
    "    - `seq_len` defines length of a sentence == how many consecutive tokens are we considered as a single sentence == how many words are in between this batch and the next batch\n",
    "    - `bptt` is how many tokens our RNN will backprop through before it's forgotten == length of RNN loop\n",
    "3. For the same row in a batch, texts are streamed from batch to batch (i.e. the texts are continuing on the same row from this to the next batch)\n",
    "4. Option to shuffle texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LM_PreLoader():\n",
    "    'prepare next word prediction (x, y) batches'\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data = data\n",
    "        self.bs = bs # number of sentences to work on at the same time\n",
    "        self.bptt = bptt # backprop through time = # of tokens our RNN will backprop through before it's forgotten \n",
    "        self.shuffle = shuffle\n",
    "        # --> batch shape = (bs, bptt)\n",
    "        total_len = sum([len(t) for t in data.x]) # total number of tokens in all data\n",
    "        self.n_batch = total_len//bs # number of tokens in a \"sentence (batch)\"\n",
    "        self.batchify() #\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.x # texts are already token indices\n",
    "        if self.shuffle:\n",
    "            texts = texts[torch.randperm(len(texts))] # shuffle\n",
    "        stream = torch.cat([torch.tensor(t) for t in texts]) # turn token indices into tensors\n",
    "        self.batched_data = stream[:self.n_batch*self.bs].view(self.bs, self.n_batch) # total_len ~= n_batch * bs, this is to be divided again when getitem\n",
    "        \n",
    "    def __len__(self): # total number of batches\n",
    "        return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''returns (x,y) pair where y is the next word of x\n",
    "        '''\n",
    "        source = self.batched_data[idx % self.bs] # row number in a batch\n",
    "        seq_idx = (idx //self.bs) * self.bptt # ???? to be understood\n",
    "        return source[seq_idx:seq_idx+self.bptt], source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'(x,y) batch maker for LM task (predict next word)\\n Sentence length = {self.n_batch}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(LM_PreLoader(ll.train, shuffle=False), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"xxbos xxmaj alan xxmaj rickman & xxmaj emma xxmaj thompson give good performances with southern / xxmaj new xxmaj orleans accents in this detective flick . xxmaj it 's worth seeing for their scenes- and xxmaj rickman 's scene with xxmaj hal xxmaj holbrook . xxmaj these three actors xxunk to entertain us no matter what the movie , it seems . xxmaj the plot for the movie shows potential\",\n",
       " \"xxmaj alan xxmaj rickman & xxmaj emma xxmaj thompson give good performances with southern / xxmaj new xxmaj orleans accents in this detective flick . xxmaj it 's worth seeing for their scenes- and xxmaj rickman 's scene with xxmaj hal xxmaj holbrook . xxmaj these three actors xxunk to entertain us no matter what the movie , it seems . xxmaj the plot for the movie shows potential ,\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1,y1 = next(iter_dl)\n",
    "vocab = proc_num.vocab # numericalization processor\n",
    "x1_text = \" \".join(vocab[o] for o in x1[0])\n",
    "y1_text = \" \".join(vocab[o] for o in y1[0])\n",
    "x1_text, y1_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(), y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"xxbos xxmaj alan xxmaj rickman & xxmaj emma xxmaj thompson give good performances with southern / xxmaj new xxmaj orleans accents in this detective flick . xxmaj it 's worth seeing for their scenes- and xxmaj rickman 's scene with xxmaj hal xxmaj holbrook . xxmaj these three actors xxunk to entertain us no matter what the movie , it seems . xxmaj the plot for the movie shows potential\",\n",
       " ') were obviously inspired by xxmaj sam xxmaj raimi . xxmaj but the camera work is a bad copy of what can be seen in \" xxmaj the xxmaj evil xxmaj dead \" and elsewhere . xxmaj some other users have written that they enjoyed the humor of this film but i did n\\'t . \\n\\n xxmaj the film rather disturbed than entertained me . xxmaj it tries to combine')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_text = \" \".join(vocab[o] for o in x1[0])\n",
    "x1_2ndrow_text = \" \".join(vocab[o] for o in x1[1])\n",
    "x1_text, x1_2ndrow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x,y) batch maker for LM task (predict next word)\n",
      " Sentence length = 421513\n"
     ]
    }
   ],
   "source": [
    "print(dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# convenience functions for language model tasks\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap everything together\n",
    "\n",
    "bs,bptt = 64,70\n",
    "data = lm_databunchify(ll, bs, bptt) # all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70]), torch.Size([70]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0][0].shape, data.train_ds[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching for classification tasks (e.g. sentiment analysis)\n",
    "- Label data by the folder they come from\n",
    "- Padding to make batches have the same sizes\n",
    "- To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:34<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:25<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start from beginning\n",
    "il = TextList.from_files(file_path, include=['train','test'])\n",
    "train_valid_splitter = partial(grandparent_splitter, valid_name='test')\n",
    "sd = SplitData.split_by_func(il, train_valid_splitter)\n",
    "ll = label_by_func(sd, parent_labeler, \n",
    "                   proc_x = [proc_tok, proc_num],\n",
    "                   proc_y = proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ll.y) # only two labels because only two classes: pos and neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledData\n",
       "x: TextList (25000 items)\n",
       "[[2, 7, 761, 27, 43, 13, 8, 138, 7, 2266, 6263, 10, 19, 31, 1002, 14, 42, 18107, 14, 16, 22, 2253, 10, 1875, 152, 2559, 14, 12, 6406, 329, 9, 24, 7, 5934, 2303, 8, 31, 51, 476, 7, 11479, 22, 3366, 10, 11, 54, 22, 12, 1048, 196, 34, 67, 833, 9, 3], [2, 7, 89, 92, 14794, 18, 10, 8, 231, 676, 142, 17, 4464, 11, 18, 276, 8, 29, 205, 759, 14, 126, 9, 215, 10, 46, 12469, 14, 114, 14794, 1396, 11, 3580, 9, 7, 1719, 88, 10, 165, 118, 676, 181, 4301, 228, 119, 46, 1829, 8, 107, 43, 10, 18, 406, 10, 1795, 29840, 66, 66, 66, 7, 166, 10, 81, 7, 560, 332, 15, 2079, 13, 10966, 8, 902, 21, 154, 17, 7, 5871, 411, 73, 46, 1019, 175, 43, 13, 151, 18101, 13526, 13, 12, 29, 66, 21, 7, 14794, 5965, 78, 37, 42, 1256, 12, 97, 29, 10, 17, 212, 16, 78, 37, 42, 76, 1256, 48, 9179, 13, 12, 97, 29, 10, 28, 16, 475, 72, 14, 68, 93, 20, 9, 7, 26, 18, 3180, 14, 866, 172, 16, 10, 18, 1966, 20, 77, 543, 676, 14, 8906, 10, 11, 18, 3598, 27060, 20, 8, 139, 261, 51, 8, 1588, 73, 7476, 163, 8, 264, 11, 514, 88, 9, 87, 46, 83, 122, 20, 4838, 8, 100, 49, 85, 15665, 17, 8, 102, 118, 72, 26, 172, 46, 90, 200, 51, 8, 1334, 1284, 73, 114, 8, 29, 36, 27, 8, 2453, 187, 11329, 33, 120, 146, 66, 1224, 10, 18, 73, 132, 1508, 2708, 19, 29, 10, 18, 406, 10, 54, 38, 8315, 760, 14, 186, 199, 20, 3227, 89, 9, 3], [2, 7, 11841, 50, 7, 19, 43, 25, 12, 245, 1380, 14, 866, 163, 9, 7, 16, 60, 12, 1026, 11, 1134, 853, 10, 30, 16, 44, 289, 14, 584, 51, 54, 9, 7, 3324, 7, 15987, 15, 240, 235, 6441, 11, 640, 10, 11, 39, 91, 37, 4188, 17, 19, 43, 9, 7, 8959, 7, 15510, 7, 19626, 11, 7, 319, 7, 11049, 659, 17, 1245, 549, 375, 9, 7, 1832, 7, 4544, 11, 7, 1957, 7, 9893, 10, 782, 414, 10, 275, 16, 116, 143, 8, 364, 9, 18, 140, 20, 22, 8, 116, 8, 534, 86, 430, 10, 11, 16, 22, 266, 14, 1688, 171, 10, 69, 8, 246, 11, 170, 41, 111, 58, 161, 6077, 9, 7, 63, 32, 200, 181, 14, 41, 3317, 121, 10, 20, 22, 603, 10, 30, 32, 207, 3234, 41, 67, 830, 14, 114, 16, 178, 9, 7, 16, 87, 35, 149, 9, 7, 504, 260, 2175, 51, 19, 29, 63, 45, 44, 631, 9, 3], [2, 18, 160, 131, 65, 2036, 118, 17, 77, 136, 10, 30, 19, 43, 225, 42, 782, 8, 70, 272, 9, 7, 12573, 1364, 1353, 830, 36, 7528, 10, 20, 15, 33, 9, 24, 7, 90, 831, 127, 4131, 10, 8, 7, 42477, 36, 7, 1403, 7, 1531, 7, 10217, 26, 7, 36480, 7, 5193, 11, 7, 10277, 7, 6029, 26, 7, 22835, 33, 11, 8, 7, 0, 36, 7, 2099, 7, 7508, 26, 7, 3277, 11, 7, 12129, 7, 10525, 26, 7, 1923, 33, 17, 43, 524, 34, 12, 36449, 14, 7, 5295, 9, 24, 7, 687, 10, 7, 36480, 15, 35, 117, 675, 27, 40, 373, 23, 136, 10, 52, 39, 9989, 34, 40, 331, 27, 7, 1923, 10, 49, 22, 0, 280, 14, 41, 12, 973, 9, 7, 3277, 10, 1872, 10, 15, 35, 117, 273, 63, 40, 15139, 15, 603, 52, 39, 22, 402, 16, 4908, 47, 12, 908, 9, 24, 7, 166, 10, 18, 85, 369, 8, 162, 13, 8, 290, 10, 30, 77, 967, 87, 35, 10, 11, 71, 42119, 61, 119, 59, 890, 249, 95, 62, 455, 1160, 10, 71, 22, 2021, 1896, 148, 72, 2847, 30, 56, 590, 197, 35, 41, 120, 973, 22, 62, 9, 7, 1566, 34, 10, 26, 19, 29, 15, 869, 4, 224, 1413, 743, 9, 7, 26, 89, 26, 368, 9, 7, 11, 640, 9, 7, 3779, 9, 7, 2407, 9, 7, 407, 9, 24, 7, 48, 512, 13, 109, 94, 19, 29, 4334, 9, 7, 8, 127, 4131, 286, 61, 17, 79, 219, 524, 915, 7, 13626, 10, 69, 46, 38, 2399, 47, 263, 586, 23, 3648, 10, 26, 46, 87, 35, 544, 45, 12, 544, 23, 1965, 9, 7, 8, 462, 140, 274, 102, 11, 453, 17622, 8, 127, 3648, 27, 12, 438, 13, 3233, 9, 7, 37, 81, 15, 19, 150, 1119, 11, 37, 638, 36, 55, 76, 3526, 33, 28, 8, 29, 10, 16, 325, 59, 374, 249, 50, 7, 16, 22, 57, 703, 11, 703, 11, 703, 10, 229, 144, 181, 1276, 9, 24, 18, 152, 41, 14, 9573, 553, 59, 8, 291, 172, 9, 7, 687, 10, 7, 3277, 13256, 281, 710, 202, 54, 52, 39, 15, 8, 357, 13, 8, 520, 9, 7, 109, 128, 234, 91, 20, 596, 17, 8, 118, 92, 371, 158, 184, 50, 7, 8, 4625, 331, 1137, 15622, 56, 590, 11, 138, 444, 28, 281, 8, 1564, 11, 46, 44, 434, 3141, 144, 119, 9, 7, 12283, 9, 24, 7, 138, 150, 13, 8, 29, 15, 228, 45, 8, 148, 10, 27, 12, 387, 13, 683, 13, 8, 7, 1672, 7, 8518, 9, 7, 154, 96, 35, 46, 57, 409, 8, 378, 34, 20, 28, 1158, 249, 66, 24, 7, 43, 73, 533, 68, 51, 19, 196, 36, 284, 7, 10217, 83, 518, 33, 10, 30, 32, 197, 35, 83, 1688, 111, 9, 7, 937, 10, 914, 34, 32, 50, 24, 381, 124, 183, 9, 3], [2, 21, 7, 22228, 21, 1116, 8, 6344, 13, 127, 462, 11, 127, 11600, 17, 12, 2413, 7, 12108, 27, 8, 689, 1178, 3828, 34, 8, 2573, 0, 16, 202, 8, 5687, 28, 8, 1947, 27, 43387, 13110, 14, 8, 364, 13, 79, 524, 9, 7, 1934, 36, 7, 1454, 7, 12329, 13, 21, 7, 341, 7, 2370, 21, 33, 15, 1801, 11, 40, 967, 7, 3301, 36, 7, 6862, 7, 24081, 13, 21, 7, 8403, 7, 0, 6449, 10, 153, 7, 1934, 22, 1140, 615, 10, 7, 2017, 36, 7, 3638, 7, 2067, 7, 44535, 13, 21, 7, 23996, 21, 33, 11, 40, 444, 189, 37, 626, 967, 189, 7, 2279, 36, 7, 3437, 0, 13, 21, 7, 8, 7, 1588, 263, 21, 33, 14158, 8, 15000, 9, 7, 19, 13841, 13, 32538, 121, 38, 591, 17, 12, 2040, 9, 7, 687, 10, 12, 26263, 36448, 189, 80, 1122, 23, 992, 198, 1122, 23, 26819, 7, 2102, 7, 14884, 11, 7, 605, 7, 14884, 404, 201, 450, 180, 59, 189, 60, 9787, 7, 796, 9, 7, 1948, 10, 8, 610, 13, 4865, 23703, 142, 45, 243, 3528, 249, 20, 73, 41, 16738, 202, 19, 2968, 2579, 59, 109, 1844, 14481, 17, 12, 3519, 11, 442, 79, 220, 272, 3750, 9, 24, 7, 19, 5538, 3238, 433, 32, 8, 0, 11, 115, 32, 8906, 11, 717, 9, 7, 110, 1301, 1150, 209, 1632, 165, 482, 6216, 13110, 14, 79, 3366, 11, 1673, 47, 12, 1562, 4288, 1210, 9, 7, 2620, 10, 19, 22409, 3291, 12, 1793, 357, 10, 7, 1251, 36, 7, 1590, 7, 26754, 13, 21, 7, 7828, 7, 0, 40, 1026, 139, 548, 7, 9715, 36, 7, 0, 7, 0, 13, 21, 7, 1309, 13, 8, 7, 436, 21, 33, 14722, 8, 5687, 27, 79, 18902, 9, 7, 1934, 34918, 209, 7, 1251, 69, 39, 518, 14, 50500, 111, 10, 30, 17, 8, 1631, 10, 8, 2909, 3792, 17, 79, 7, 12108, 58977, 11, 46, 1917, 72, 34, 2539, 9, 7, 7954, 10, 46, 4503, 12, 1217, 27, 7, 1251, 119, 46, 8131, 7, 9715, 72, 17, 8, 5474, 13, 8, 18902, 9, 7, 71, 2772, 12, 2335, 143, 56, 3366, 11, 1673, 11, 16, 15, 49632, 27, 557, 9, 7, 1251, 60, 570, 20, 4611, 38, 25431, 12813, 100, 45, 12, 1498, 11, 46, 431, 14, 16, 9, 7, 1095, 10, 1843, 60, 8159, 14, 7, 1251, 9, 7, 8, 1498, 10702, 15, 733, 8, 247, 387, 13, 372, 65, 7, 23137, 23, 7, 4132, 20, 105, 299, 111, 61, 13, 79, 4723, 9, 7, 8, 4029, 87, 37, 4413, 79, 3105, 9, 7, 1301, 345, 17, 507, 15, 352, 9, 7, 2279, 518, 229, 1007, 14, 98, 12, 9792, 1297, 34, 191, 1663, 9, 7, 1251, 2425, 20, 54, 15, 74, 445, 28, 40, 548, 11, 39, 1578, 8, 3766, 13841, 2313, 40, 18902, 11, 213, 142, 9, 24, 7, 864, 10, 21, 7, 22228, 21, 8141, 26, 12, 7950, 2129, 29, 59, 8, 309, 13, 12, 36448, 34, 666, 2157, 100, 49, 14481, 104, 8305, 7807, 14, 2860, 530, 9, 7, 46, 2063, 12, 658, 1443, 11, 21812, 209, 34, 12, 6104, 285, 380, 175, 666, 141, 72, 17, 3298, 11, 6216, 27, 12428, 23, 211, 22802, 9, 7, 985, 10, 278, 3108, 1942, 14, 1025, 229, 402, 343, 10, 30, 7, 1934, 60, 12, 2360, 69, 39, 240, 748, 104, 8, 996, 27, 12, 4635, 3605, 9, 7, 867, 10, 46, 1803, 20, 43, 13, 111, 60, 442, 5519, 9, 7, 330, 10, 26, 46, 38, 59, 14, 504, 61, 13, 2441, 10, 7, 1934, 9823, 8, 5687, 53, 7, 1251, 87, 45, 8, 8882, 9, 7, 2017, 518, 14, 544, 12, 2065, 13, 941, 7, 1618, 366, 1801, 8, 524, 9, 7, 2017, 1667, 20, 40, 2847, 331, 15, 59, 14, 221, 2631, 11, 39, 789, 79, 365, 9, 7, 1934, 2771, 7504, 14, 8, 1917, 11, 15220, 260, 45, 8, 2131, 27, 40, 9041, 7285, 69, 46, 5394, 14, 365, 111, 9, 7, 1934, 4527, 12, 13874, 17, 8, 3797, 51, 8, 7827, 10, 30, 39, 1236, 56, 9, 24, 0, 21, 15, 37, 12, 3233, 198, 6754, 29, 20, 32, 78, 370, 451, 142, 55, 451, 27, 106, 8, 470, 15, 2254, 702, 23, 3235, 9, 7, 47, 8, 148, 13, 19, 0, 29, 10, 278, 1503, 41, 659, 104, 1785, 49, 81, 478, 81, 28, 530, 11, 79, 4892, 9, 7, 1454, 7, 12329, 179, 205, 48, 1453, 26, 268, 23, 1753, 7, 1934, 11, 40, 6177, 262, 15, 8, 81, 313, 14, 2806, 27, 19, 14316, 10, 153, 8, 81, 102, 89, 23, 558, 551, 10, 7, 6862, 7, 24081, 10, 15, 11167, 14, 48, 10988, 967, 232, 9, 7, 26, 7, 3301, 10, 71, 179, 1620, 8, 1438, 13, 793, 5137, 14, 12, 1657, 139, 261, 11, 4240, 48, 398, 1727, 9, 7, 16, 15, 12, 5117, 14, 7, 12329, 22, 262, 20, 39, 78, 671, 40, 123, 14, 8, 241, 13, 1510, 332, 177, 414, 9, 7, 2032, 10, 7, 12329, 60, 8, 81, 232, 20, 433, 108, 8, 1269, 14, 1477, 12, 43, 23, 11336, 51, 675, 23, 159, 23, 1850, 230, 14, 9636, 230, 9, 24, 7, 8, 127, 992, 38, 7, 1897, 975, 10, 11, 46, 132, 304, 8, 7246, 5222, 9, 7, 251, 54, 15, 74, 2115, 17, 1787, 10, 21, 7, 22228, 21, 8363, 104, 8414, 9, 21, 7, 58023, 21, 4341, 7, 28271, 7, 0, 91, 12, 835, 320, 27, 40, 5187, 32733, 11, 26, 14222, 26, 19, 1178, 1300, 496, 10, 7, 0, 179, 16, 185, 53, 12, 4005, 31, 9, 7, 1213, 51, 8, 610, 13, 12, 675, 291, 55, 8613, 17, 120, 293, 13, 8, 674, 10, 21, 7, 22228, 21, 2506, 106, 16, 15, 52, 2222, 2968, 9, 7, 8, 150, 69, 8, 7, 1044, 7220, 2856, 7, 2017, 21213, 72, 8, 110, 780, 10, 30, 76, 16, 96, 41, 99, 4269, 9, 7, 495, 10, 8, 7, 14884, 975, 58, 37, 2360, 72, 370, 94, 1113, 55, 780, 9, 7, 47, 5924, 23, 61, 10, 32, 83, 58, 35, 478, 64, 574, 14, 1775, 9, 3], [2, 18, 85, 99, 282, 949, 14, 335, 19, 31, 28, 12, 217, 75, 10, 119, 335, 21, 7, 1020, 14, 7, 5223, 10, 21, 80, 18, 276, 14, 42, 2475, 9, 18, 25, 52, 688, 9, 7, 8, 110, 827, 169, 59, 16, 25, 8, 4885, 291, 80, 18, 122, 25, 4027, 2413, 51, 21, 7, 1020, 14, 7, 5223, 9, 21, 24, 7, 295, 18, 25, 1032, 117, 94, 9, 24, 7, 34, 8, 1188, 510, 7, 7658, 10, 7, 9345, 11, 7, 12882, 86, 835, 17, 79, 593, 9, 7, 284, 18, 87, 1151, 53, 7, 7658, 22, 123, 11, 107, 11, 276, 56, 70, 640, 9, 24, 18, 96, 1151, 84, 255, 827, 59, 8, 31, 9, 7, 16, 96, 13, 99, 244, 52, 94, 146, 10, 28, 512, 54, 96, 13, 99, 68, 4739, 34, 8, 1084, 4521, 11, 8, 1980, 204, 9, 7, 16, 25, 117, 15459, 12, 29, 14, 42, 827, 9, 7, 54, 96, 103, 13, 99, 68, 211, 11, 1113, 24, 7, 8, 138, 169, 59, 19, 31, 15, 8, 21, 1620, 21, 291, 9, 18, 96, 1151, 13, 9352, 20, 9, 7, 30, 47, 20, 75, 18, 83, 87, 1151, 478, 64, 580, 14, 111, 9, 24, 383, 124, 183, 3], [2, 7, 0, 33, 229, 1171, 15, 1249, 37, 631, 17, 8, 164, 195, 30, 17, 8, 195, 13, 7, 361, 5657, 10, 37, 81, 15, 16, 631, 10, 16, 22, 2611, 9, 7, 8, 7, 2347, 60, 99, 386, 8, 1263, 2166, 7283, 13, 12, 3453, 23, 730, 4341, 23, 2921, 23, 170, 49, 301, 1203, 2183, 36, 7, 5191, 815, 7, 0, 7, 2176, 33, 30, 60, 251, 1051, 12, 3342, 13, 219, 353, 1237, 36, 7, 15610, 10, 7, 2176, 263, 33, 9, 24, 7, 303, 38, 8709, 17, 12, 29, 13, 19, 555, 11, 46, 327, 14, 956, 16, 9, 7, 6866, 7, 8334, 11, 7, 50399, 7, 9739, 23, 7, 1185, 509, 53, 46, 673, 46, 86, 1750, 30, 17, 19, 31, 9, 7, 11561, 7, 2067, 179, 48, 600, 14, 721, 158, 14, 8, 4056, 30, 847, 20, 158, 253, 42, 15, 1665, 251, 8, 246, 803, 53, 334, 13, 16, 15, 1000, 9, 7, 715, 57, 596, 10, 67, 11, 97, 2984, 141, 72, 27, 74, 10377, 55, 313, 11, 115, 8, 82, 57, 654, 27, 12, 110, 6341, 702, 23, 1635, 1023, 14, 5028, 72, 8, 922, 1158, 249, 13, 18311, 9, 24, 7, 54, 83, 15, 35, 76, 313, 14, 84, 19, 28, 8, 309, 251, 90, 44, 140, 20, 255, 78, 42, 299, 34, 277, 166, 9, 7, 154, 37, 126, 309, 17, 8, 2350, 13, 12, 67, 82, 322, 13, 57, 28, 79, 220, 2140, 66, 3], [2, 7, 19, 452, 676, 61, 27, 67, 3185, 10, 7, 6174, 8, 1645, 61, 14, 2251, 8, 2966, 13, 17086, 10, 11, 7, 11349, 15, 1026, 26, 677, 17, 56, 232, 9, 7, 16, 44, 748, 997, 119, 20, 10, 16, 22, 147, 767, 7, 361, 697, 166, 10, 754, 34, 12, 21654, 27, 326, 309, 9531, 10, 5079, 120, 265, 13, 1186, 10, 2444, 55, 1304, 9, 7, 17, 102, 689, 10, 58, 35, 472, 147, 75, 168, 19, 9, 7, 98, 8, 0, 1916, 27, 7, 319, 0, 26, 8, 7, 3594, 7, 145, 322, 10, 103, 1201, 7, 4493, 7, 18067, 9, 7, 166, 20, 25, 67, 10, 11, 23557, 7, 5851, 15, 89, 2701, 10, 1027, 27, 19, 944, 9, 3], [2, 18, 2718, 14, 84, 19, 29, 106, 16, 207, 12, 67, 585, 149, 34, 946, 9, 7, 30, 12, 188, 13, 100, 370, 41, 83, 354, 1347, 55, 305, 22, 99, 13897, 8, 585, 9, 24, 7, 370, 116, 16, 25, 12, 164, 1478, 9, 7, 8, 29, 15, 626, 26, 395, 11, 248, 4381, 26, 8, 440, 73, 1508, 9, 7, 54, 83, 15, 74, 313, 14, 221, 12, 2700, 13, 8, 130, 23, 30, 149, 289, 95, 16, 468, 53, 305, 85, 99, 553, 95, 21, 7, 73, 35, 16, 42, 595, 14, 114, 12, 29, 135, 54, 86, 4248, 34, 12, 1572, 66, 7, 11, 115, 8, 4248, 28, 65, 313, 73, 159, 957, 11, 392, 5886, 11, 538, 66, 50, 66, 21, 7, 11, 20, 22, 59, 16, 50, 7, 8, 130, 15, 1562, 11, 4081, 9, 7, 8, 4248, 38, 97, 1511, 36, 30, 16, 179, 293, 14, 611, 9994, 34, 12, 29, 20, 74, 43, 17, 79, 228, 351, 105, 405, 14, 271, 50, 33, 9, 7, 8, 137, 15, 354, 10, 11, 44, 100, 38, 1372, 8292, 2210, 9, 24, 7, 14, 2798, 16, 72, 95, 7, 16, 22, 43, 13, 8, 272, 118, 18, 160, 144, 131, 23, 786, 260, 50, 3], [2, 7, 19, 31, 60, 14, 42, 8, 272, 18, 41, 144, 131, 9, 7, 8, 440, 13, 8, 31, 40431, 8, 329, 104, 553, 54, 295, 445, 9, 7, 8, 82, 367, 13, 8, 31, 15, 1393, 45, 138, 10, 27, 8, 137, 52, 354, 32, 57, 41, 14, 3393, 9, 7, 8, 440, 62, 7, 1056, 7, 3051, 62, 8733, 12, 28428, 13, 1405, 69, 17, 212, 54, 38, 1423, 17, 942, 9, 7, 19, 78, 37, 42, 44112, 26, 12, 204, 31, 712, 14, 8, 2998, 13, 903, 1021, 22, 238, 69, 8, 1405, 62, 1326, 62, 9, 7, 8, 1405, 182, 703, 11, 509, 53, 424, 3394, 17, 8, 31, 27, 8, 81, 1555, 129, 8, 114, 72, 80, 307, 53, 158, 61, 12, 7, 1078, 7, 0, 397, 9, 7, 63, 32, 144, 98, 8, 613, 14, 807, 19, 31, 115, 58, 52, 10, 115, 3227, 8, 994, 9, 3]...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb\n",
       "y: ItemList (25000 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/xianli/Desktop/fast/Part2/data/imdb"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ll, open(data_dir/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxbos xxmaj well ... tremors i , the original started off in 1990 and i found the movie quite enjoyable to watch . however , they proceeded to make tremors ii and iii . xxmaj trust me , those movies started going downhill right after they finished the first one , i mean , ass blasters ? ? ? xxmaj now , only xxmaj god himself is capable of answering the question \" why in xxmaj gods name would they create another one of these dumpster dives of a movie ? \" xxmaj tremors iv can not be considered a bad movie , in fact it can not be even considered an epitome of a bad movie , for it lives up to more than that . xxmaj as i attempted to sit though it , i noticed that my eyes started to bleed , and i hoped profusely that the little girl from the ring would crawl through the tv and kill me . did they really think that dressing the people who had stared in the other movies up as though they we \\'re from the wild west would make the movie ( with the exact same occurrences ) any better ? honestly , i would never suggest buying this movie , i mean , there are cheaper ways to find things that burn well . xxeos',\n",
       "  'neg'),\n",
       " (\"xxbos i saw this film a while back and it 's still at the top of my ' favorite movies ' list . xxmaj it is amazingly put together and what really makes the film are the detailed tid bits ( such as the ' xxmaj cafe xxmaj xxunk ' coffee crate being reused as a cup to wash her grandsons hair ) that people are n't seeing because you will not understand this movie unless you are hispanic . xxmaj this is just one of those films that is very culturally specific and particular . xxmaj please do not bash this film if you have no prior knowledge of what foundation it 's being built upon . i completely see what the writer / director was going for , and he hit the target perfectly ! xxmaj this film is highly deserving of a better rating . xxeos\",\n",
       "  'pos')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### samplers\n",
    "- ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler): # for validation set\n",
    "    ''' Get indices of docs that is reverse-sorted by key (e.g. get \n",
    "    indices of the documents from longest to shortest)'''\n",
    "    def __init__(self, data_source, key):\n",
    "        self.data_source = data_source\n",
    "        self.key = key\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    def __iter__(self): # feed longest text first, return indices of the correspond texts\n",
    "        return iter(sorted(list(range(len(self.data_source))), key = self.key, reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size `50 * bs`. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.\n",
    "\n",
    "Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# ??? needs to read more carefully\n",
    "class SortishSampler(Sampler):\n",
    "    '''\n",
    "    Note: key is a callable function\n",
    "    '''\n",
    "    def __init__(self, data_source,key, bs):\n",
    "        self.data_source = data_source\n",
    "        self.key = key\n",
    "        self.bs = bs\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)] # 50 times bigger megabatch\n",
    "        sorted_idx = torch.cat([torch.tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches]) # reverse-sort the megabatches by key\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)] # extract batch indices from megabatch indices\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0], batches[max_idx] = batches[max_idx],batches[0] # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2) #excluding begin and end\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else torch.LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "- Padding token has id = 1 by default (see default_spec_tok), remember to change the pad_idx if this is changed!\n",
    "- Pad each sequence **in the end** so that they all have same size when batching them together\n",
    "- Padding in the end in order to use Pytorch convenience functions in future that allows us to ignore the padding\n",
    "- Pad all docs to the length of the longest doc\n",
    "- Longest sequences are in the first batch, all other batches are shuffled but they have similar lengths inside each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    ''' '''\n",
    "    max_len = max([len(s[0]) for s in samples]) # s[0] is x, s[1] is y\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx # a giant matrix with all elements = pad_idx\n",
    "    for i, s in enumerate(samples):\n",
    "        if pad_first:\n",
    "            res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:\n",
    "            res[i, :len(s[0])] = torch.LongTensor(s[0])\n",
    "    return res, torch.tensor([s[1] for s in samples]) # (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "# sampler is a generator\n",
    "train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs = bs) # key = doc length, ll.train[int(t)] == doc #t\n",
    "train_dl = DataLoader(ll.train, batch_size=bs, sampler = train_sampler, collate_fn=pad_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3311]), torch.Size([64]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_dl = iter(train_dl)\n",
    "x,y = next(iter_dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3311, 1699, 1577, 1554, 1417], 1057)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the texts after removing the padding\n",
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([175, 175, 175, 175, 175], 171)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the other batches are random, but inside each batch the doc sizes are similar \n",
    "# (will be pad to max doc size)\n",
    "x,y = next(iter_dl)\n",
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    7, 3938,  ..., 1014,   92,    3],\n",
       "        [   2,    7,    8,  ..., 7250,    9,    3],\n",
       "        [   2,    7,   19,  ...,   29,   92,    3],\n",
       "        ...,\n",
       "        [   2,    7,   16,  ...,    1,    1,    1],\n",
       "        [   2,   18,   87,  ...,    1,    1,    1],\n",
       "        [   2,   18,  160,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exp.nb_XLibrary.DataBunch at 0x1a1fc94160>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python notebook2script.py test_preprocessing_Lesson6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
