{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce training from beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup path and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import gzip\n",
    "import pickle\n",
    "# from fastai.datasets import download_data\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST_url = 'http://deeplearning.net/data/mnist/mnist.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/xianli/Desktop/fast/Part2'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data'),\n",
       " PosixPath('/Users/xianli/Desktop/fast/Part2/data/mnist.pkl.gz'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_path = Path('.').resolve()\n",
    "data_path = home_path/'data'\n",
    "file_name = data_path/'mnist.pkl.gz'\n",
    "\n",
    "home_path, data_path, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open(file_name,'r') as f:\n",
    "    ((x_train,y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    \n",
    "x_train,y_train, x_valid, y_valid = map(torch.tensor, (x_train,y_train, x_valid, y_valid))\n",
    "\n",
    "type(x_train), x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, fan_in, nh, fan_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(fan_in, nh), nn.ReLU(), nn.Linear(nh, fan_out)] # initialization done inside layer definitions\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in = 28**2\n",
    "nh = 50\n",
    "fan_out = 10\n",
    "\n",
    "model = Model(fan_in, nh, fan_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model(x_train)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.max(-1,keepdim=True)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropyLoss\n",
    "- logits z needs to be converted to probabilities --> softmax --> log-likelihood: logsoftmax\n",
    "- calculate cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): return x.exp()/(x.exp().sum(-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x): \n",
    "    a = x.max(-1,keepdim=True)[0]\n",
    "    return a+(x-a).exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if our logsumexp is correct\n",
    "torch.allclose(logsumexp(z),z.logsumexp(-1,keepdim=True),rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):  \n",
    "    return x-logsumexp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 10]), torch.Size([50000, 1]), torch.Size([50000, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(z).shape, logsumexp(z).shape, log_softmax(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log likelihood loss == cross entropy loss\n",
    "# there is a pytorch function F.nll_loss for this\n",
    "def nll(log_p_pred, y):\n",
    "    # p_pred is essentially one-hot encoded. In MNIST case, y values directly correspond to the indices\n",
    "    return -log_p_pred[range(y.shape[0]),y].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2999, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pred = log_softmax(z)\n",
    "our_loss = nll(p_pred, y_train)\n",
    "our_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch cross entropy loss will automatically convert logits into probability and calculate negative log-likehood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3000, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard cross entropy loss from pytorch\n",
    "standard_loss = F.cross_entropy(z, y_train)\n",
    "standard_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(our_loss, standard_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor\n",
    "def my_CrossEntropy_MNIST(pred, targ):\n",
    "    log_z = z - z.logsumexp(-1, keepdim=True)\n",
    "    return -log_z[range(targ.shape[0]),targ].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(F.cross_entropy(z, y_train), my_CrossEntropy_MNIST(z, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic version of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = \"last_expr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy # note nn.CrossEntropyLoss() is a class but F.crossentropy() is a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, targ):\n",
    "    return (torch.argmax(pred, dim=1)==targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "lr = 0.1\n",
    "epoch = 2\n",
    "n = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch #1\n",
      "Current training accuracy = 0.109375\n",
      "Current training accuracy = 0.750000\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.828125\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.953125\n",
      "Begin epoch #2\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "model = Model(fan_in, nh, fan_out)\n",
    "\n",
    "for ep in range(epoch):\n",
    "    print('Begin epoch #%d' %(ep+1))\n",
    "    for i in range((n-1)//bs+1): # loop by mini-batch\n",
    "        # get batch\n",
    "        batch_start = i*bs\n",
    "        batch_end = batch_start+bs\n",
    "        x_batch = x_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "        \n",
    "        # generate prediction\n",
    "        pred = model(x_batch) \n",
    "        # get loss\n",
    "        loss = loss_func(pred, y_batch)\n",
    "        if i%50 == 0:\n",
    "            print('Current training accuracy = %f' %accuracy(pred, y_batch))\n",
    "\n",
    "        # backprop and sdg\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l,'weight'):\n",
    "                    l.weight -= lr*l.weight.grad\n",
    "                    l.bias -= lr*l.bias.grad\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplifcation 1: build model.parameters() with `__setattr__`\n",
    "- register the name of the attribute whenever one is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self,dim_in,nh,dim_out):\n",
    "        self._modules = {} # a dictionary to store the attribute names, __dict__ is the built-in one\n",
    "        self.lin1 = nn.Linear(dim_in,nh)\n",
    "        self.lin2 = nn.Linear(nh, dim_out)\n",
    "    \n",
    "    def __setattr__(self, name, val):\n",
    "        if not name.startswith('_'): \n",
    "            self._modules[name] = val # record the attribute name\n",
    "        super().__setattr__(name,val) # override the default setattr\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self._modules}\"\n",
    "    \n",
    "    def parameters(self): # a generator\n",
    "        for l in self._modules.values(): # iter through the dictionary, note values() should be a function \n",
    "            for p in l.parameters():\n",
    "                yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin1': Linear(in_features=784, out_features=50, bias=True), 'lin2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DummyModule(784,50,10)\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in dm.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): # model.parameters() won't work if the layers are not attributes of the object\n",
    "    def __init__(self, fan_in,nh,fan_out):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(fan_in,nh)\n",
    "        self.lin2 = nn.Linear(nh,fan_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.lin2(F.relu(self.lin1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lin1): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "[torch.Size([50, 784]), torch.Size([50]), torch.Size([10, 50]), torch.Size([10])]\n"
     ]
    }
   ],
   "source": [
    "fan_in = 28**2\n",
    "nh = 50\n",
    "fan_out = 10\n",
    "model = Model(fan_in, nh, fan_out)\n",
    "print(model)\n",
    "print([p.shape for p in model.parameters()])\n",
    "\n",
    "def fit():\n",
    "    for ep in range(epoch):\n",
    "        print('Begin epoch #%d' %(ep+1))\n",
    "        for i in range((n-1)//bs+1): # loop by mini-batch\n",
    "            # get batch\n",
    "            batch_start = i*bs\n",
    "            batch_end = batch_start+bs\n",
    "            x_batch = x_train[batch_start:batch_end]\n",
    "            y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "            # generate prediction\n",
    "            pred = model(x_batch) \n",
    "            # get loss\n",
    "            loss = loss_func(pred, y_batch)\n",
    "            if i%50 == 0:\n",
    "                print('Current training accuracy = %f' %accuracy(pred, y_batch))\n",
    "\n",
    "            # backprop and sdg\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= lr*p.grad\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch #1\n",
      "Current training accuracy = 0.109375\n",
      "Current training accuracy = 0.781250\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.828125\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.953125\n",
      "Begin epoch #2\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.843750\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.968750\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `self.add_module` to register layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(fan_in, nh), nn.ReLU(), nn.Linear(nh, fan_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(layers):\n",
    "            self.add_module(f'Layer_{i}', l)\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2(\n",
       "  (Layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (Layer_1): ReLU()\n",
       "  (Layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Model2(layers)\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register automatically using `nn.ModuleList`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model3(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Model3(layers)\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(fan_in,nh),nn.ReLU(),nn.Linear(nh,fan_out))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch #1\n",
      "Current training accuracy = 0.109375\n",
      "Current training accuracy = 0.781250\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.828125\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.953125\n",
      "Begin epoch #2\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.953125\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.906250\n",
      "Current training accuracy = 0.937500\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.921875\n",
      "Current training accuracy = 0.953125\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pytorch optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "model = nn.Sequential(nn.Linear(fan_in,nh),nn.ReLU(),nn.Linear(nh,fan_out))\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for ep in range(epoch):\n",
    "        print('Begin epoch #%d' %(ep+1))\n",
    "        for i in range((n-1)//bs+1): # loop by mini-batch\n",
    "            # get batch\n",
    "            batch_start = i*bs\n",
    "            batch_end = batch_start+bs\n",
    "            x_batch = x_train[batch_start:batch_end]\n",
    "            y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "            # generate prediction\n",
    "            pred = model(x_batch) \n",
    "            # get loss\n",
    "            loss = loss_func(pred, y_batch)\n",
    "            if i%50 == 0:\n",
    "                print('Current training accuracy = %f' %accuracy(pred, y_batch))\n",
    "\n",
    "            # backprop and sdg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch #1\n",
      "Current training accuracy = 0.015625\n",
      "Current training accuracy = 0.203125\n",
      "Current training accuracy = 0.390625\n",
      "Current training accuracy = 0.546875\n",
      "Current training accuracy = 0.640625\n",
      "Current training accuracy = 0.640625\n",
      "Current training accuracy = 0.781250\n",
      "Current training accuracy = 0.796875\n",
      "Current training accuracy = 0.750000\n",
      "Current training accuracy = 0.828125\n",
      "Current training accuracy = 0.828125\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.781250\n",
      "Current training accuracy = 0.906250\n",
      "Begin epoch #2\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.781250\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.843750\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.875000\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.890625\n",
      "Current training accuracy = 0.859375\n",
      "Current training accuracy = 0.968750\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader\n",
    "- Dataset base class just allows basic operations like indexing and len\n",
    "- Dataloader yields data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i], self.y[i]\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset(x_train, y_train)\n",
    "ds_valid = Dataset(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train), len(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([5]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = ds_train[:5]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs=64):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb shape = torch.Size([10001, 784]), yb shape = torch.Size([10001])\n",
      "xb shape = torch.Size([10001, 784]), yb shape = torch.Size([10001])\n",
      "xb shape = torch.Size([10001, 784]), yb shape = torch.Size([10001])\n",
      "xb shape = torch.Size([10001, 784]), yb shape = torch.Size([10001])\n",
      "xb shape = torch.Size([9996, 784]), yb shape = torch.Size([9996])\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(ds_train, bs=10001)\n",
    "for o in loader:\n",
    "    xb, yb = o\n",
    "    print(f'xb shape = {xb.shape}, yb shape = {yb.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(ds_valid, bs = 64)\n",
    "val_gen = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b53ac88>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xv, yv = next(val_gen)\n",
    "plt.imshow(xv[0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rewrite the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in, nh, fan_out = 784, 50, 10\n",
    "lr = 0.05\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(fan_in,nh),nn.ReLU(),nn.Linear(nh,fan_out))\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "bs = 64\n",
    "ds_train = Dataset(x_train, y_train)\n",
    "ds_loader = DataLoader(ds_train, bs)\n",
    "model, opt = get_model()\n",
    "\n",
    "def fit():\n",
    "    for ep in range(epochs):\n",
    "        i = 0\n",
    "        print(f'begin epoch #{ep+1}')\n",
    "        for xb, yb in ds_loader:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            if not i % 50:\n",
    "                acc = accuracy(pred, yb)\n",
    "                print(f'Training loss = {loss.item()}, accuracy = {acc}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin epoch #1\n",
      "Training loss = 2.304140567779541, accuracy = 0.125\n",
      "Training loss = 1.673831582069397, accuracy = 0.703125\n",
      "Training loss = 0.934434711933136, accuracy = 0.8125\n",
      "Training loss = 0.6859035491943359, accuracy = 0.84375\n",
      "Training loss = 0.5768215656280518, accuracy = 0.859375\n",
      "Training loss = 0.755027174949646, accuracy = 0.8125\n",
      "Training loss = 0.48127150535583496, accuracy = 0.890625\n",
      "Training loss = 0.3903617262840271, accuracy = 0.84375\n",
      "Training loss = 0.3743698000907898, accuracy = 0.890625\n",
      "Training loss = 0.31659597158432007, accuracy = 0.90625\n",
      "Training loss = 0.39633673429489136, accuracy = 0.859375\n",
      "Training loss = 0.4195123314857483, accuracy = 0.921875\n",
      "Training loss = 0.262899786233902, accuracy = 0.9375\n",
      "Training loss = 0.3345005214214325, accuracy = 0.890625\n",
      "Training loss = 0.4275611639022827, accuracy = 0.890625\n",
      "Training loss = 0.2290865033864975, accuracy = 0.96875\n",
      "begin epoch #2\n",
      "Training loss = 0.3891805410385132, accuracy = 0.921875\n",
      "Training loss = 0.2997896671295166, accuracy = 0.890625\n",
      "Training loss = 0.28504934906959534, accuracy = 0.90625\n",
      "Training loss = 0.28516608476638794, accuracy = 0.90625\n",
      "Training loss = 0.23310358822345734, accuracy = 0.90625\n",
      "Training loss = 0.49767786264419556, accuracy = 0.84375\n",
      "Training loss = 0.36519768834114075, accuracy = 0.90625\n",
      "Training loss = 0.2391328513622284, accuracy = 0.921875\n",
      "Training loss = 0.24537473917007446, accuracy = 0.921875\n",
      "Training loss = 0.18508264422416687, accuracy = 0.953125\n",
      "Training loss = 0.3342217803001404, accuracy = 0.890625\n",
      "Training loss = 0.3488315939903259, accuracy = 0.921875\n",
      "Training loss = 0.19709813594818115, accuracy = 0.9375\n",
      "Training loss = 0.2540147304534912, accuracy = 0.921875\n",
      "Training loss = 0.3811436891555786, accuracy = 0.875\n",
      "Training loss = 0.19074614346027374, accuracy = 0.96875\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich dataloader: random sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(): # only spit out indices, not real data, to save space\n",
    "    def __init__(self, ds, bs=bs, shuffle=False):\n",
    "        self.n = len(ds)\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            idxs = torch.randperm(self.n)\n",
    "        else:\n",
    "            idxs = torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs):\n",
    "            yield idxs[i:i+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 0, 2]), tensor([4, 3, 0]), tensor([1, 3, 4]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ds = Dataset(*ds_train[:5]) # * because dataset takes two parameters\n",
    "bs = 3\n",
    "sampler = Sampler(small_ds, bs, shuffle=True)\n",
    "next(iter(sampler)),next(iter(sampler)),next(iter(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 5), (2, 4, 6))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpack a list of tuples\n",
    "tmp = [(1,2),(3,4),(5,6)]\n",
    "a, b = zip(*tmp)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data_list):\n",
    "    # data_list is a list of tuples of (x,y)\n",
    "    xt,yt = zip(*data_list)\n",
    "    return torch.stack(xt), torch.stack(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [3, 4, 5],\n",
       "         [4, 5, 6]]), tensor([2, 4, 6]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collate function test\n",
    "data_list = [([1,2,3],2),([3,4,5],4),([4,5,6],6)]\n",
    "data_list = [(torch.tensor(ele[0]),torch.tensor(ele[1])) for ele in data_list]\n",
    "collate_fn(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn = collate_fn):\n",
    "        self.ds = ds\n",
    "        self.sampler = sampler\n",
    "        self.collate_fn = collate_fn\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: # yield indices\n",
    "            yield self.collate_fn([self.ds[i] for i in s]) # our dataset class doesn't necessarily allow advanced indexing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([9, 4, 1, 3, 4]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 5\n",
    "ds_train = Dataset(x_train, y_train)\n",
    "sampler = Sampler(ds_train, bs, shuffle=True)\n",
    "loader = DataLoader(ds_train, sampler, collate_fn=collate_fn)\n",
    "gen = iter(loader)\n",
    "next(gen) # batch size of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12bac36a0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = next(gen)[0][0]\n",
    "plt.imshow(img.view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rewrite fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in, nh, fan_out = 784, 50, 10\n",
    "lr = 0.01\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(fan_in,nh),nn.ReLU(),nn.Linear(nh,fan_out))\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "bs = 64\n",
    "ds_train = Dataset(x_train, y_train)\n",
    "sampler = Sampler(ds_train, bs, shuffle=True)\n",
    "ds_loader = DataLoader(ds_train, sampler, collate_fn=collate_fn)\n",
    "model, opt = get_model()\n",
    "\n",
    "def fit():\n",
    "    for ep in range(epochs):\n",
    "        i = 0\n",
    "        print(f'begin epoch #{ep+1}')\n",
    "        for xb, yb in ds_loader:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            if not i % 50:\n",
    "                acc = accuracy(pred, yb)\n",
    "                print(f'Training loss = {loss.item()}, accuracy = {acc}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin epoch #1\n",
      "Training loss = 2.318932056427002, accuracy = 0.109375\n",
      "Training loss = 2.197685956954956, accuracy = 0.375\n",
      "Training loss = 2.121443033218384, accuracy = 0.484375\n",
      "Training loss = 1.9659712314605713, accuracy = 0.640625\n",
      "Training loss = 1.7924023866653442, accuracy = 0.640625\n",
      "Training loss = 1.62751305103302, accuracy = 0.71875\n",
      "Training loss = 1.5172816514968872, accuracy = 0.75\n",
      "Training loss = 1.3552361726760864, accuracy = 0.78125\n",
      "Training loss = 1.236562967300415, accuracy = 0.71875\n",
      "Training loss = 1.2065951824188232, accuracy = 0.75\n",
      "Training loss = 1.048667073249817, accuracy = 0.828125\n",
      "Training loss = 0.9769032001495361, accuracy = 0.828125\n",
      "Training loss = 0.8507418632507324, accuracy = 0.8125\n",
      "Training loss = 0.7453125715255737, accuracy = 0.84375\n",
      "Training loss = 0.916109561920166, accuracy = 0.75\n",
      "Training loss = 0.6023349761962891, accuracy = 0.890625\n",
      "begin epoch #2\n",
      "Training loss = 0.7895714044570923, accuracy = 0.796875\n",
      "Training loss = 0.5712854862213135, accuracy = 0.921875\n",
      "Training loss = 0.6978410482406616, accuracy = 0.859375\n",
      "Training loss = 0.52670818567276, accuracy = 0.9375\n",
      "Training loss = 0.5723998546600342, accuracy = 0.859375\n",
      "Training loss = 0.4478614926338196, accuracy = 0.921875\n",
      "Training loss = 0.47613435983657837, accuracy = 0.90625\n",
      "Training loss = 0.6549996733665466, accuracy = 0.828125\n",
      "Training loss = 0.3938969671726227, accuracy = 0.984375\n",
      "Training loss = 0.4939146339893341, accuracy = 0.875\n",
      "Training loss = 0.6665191650390625, accuracy = 0.8125\n",
      "Training loss = 0.45769789814949036, accuracy = 0.921875\n",
      "Training loss = 0.5409982204437256, accuracy = 0.84375\n",
      "Training loss = 0.3867174983024597, accuracy = 0.90625\n",
      "Training loss = 0.3928808271884918, accuracy = 0.9375\n",
      "Training loss = 0.6159292459487915, accuracy = 0.828125\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset(x_train, y_train) # our dataset class works for pytorch as well\n",
    "ds_valid = Dataset(x_valid, y_valid) # or use pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, bs, sampler = RandomSampler(ds_train), collate_fn=collate_fn)\n",
    "dl_valid = DataLoader(ds_valid, bs, sampler = SequentialSampler(ds_valid), collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or even simpler\n",
    "dl_train = DataLoader(ds_train, bs, shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "def fit():\n",
    "    for ep in range(epochs):\n",
    "        i = 0\n",
    "        print(f'begin epoch #{ep+1}')\n",
    "        for xb, yb in dl_train:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            if not i % 50:\n",
    "                acc = accuracy(pred, yb)\n",
    "                print(f'Training loss = {loss.item()}, accuracy = {acc}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin epoch #1\n",
      "Training loss = 2.288123369216919, accuracy = 0.125\n",
      "Training loss = 2.2213871479034424, accuracy = 0.296875\n",
      "Training loss = 2.120112657546997, accuracy = 0.625\n",
      "Training loss = 2.005922794342041, accuracy = 0.671875\n",
      "Training loss = 1.8612971305847168, accuracy = 0.640625\n",
      "Training loss = 1.7352294921875, accuracy = 0.734375\n",
      "Training loss = 1.5491865873336792, accuracy = 0.828125\n",
      "Training loss = 1.5020273923873901, accuracy = 0.765625\n",
      "Training loss = 1.241751790046692, accuracy = 0.765625\n",
      "Training loss = 1.057792067527771, accuracy = 0.84375\n",
      "Training loss = 1.028617024421692, accuracy = 0.8125\n",
      "Training loss = 0.8389776945114136, accuracy = 0.8125\n",
      "Training loss = 0.9260385632514954, accuracy = 0.796875\n",
      "Training loss = 0.736692488193512, accuracy = 0.921875\n",
      "Training loss = 0.651896595954895, accuracy = 0.921875\n",
      "Training loss = 0.8481499552726746, accuracy = 0.796875\n",
      "begin epoch #2\n",
      "Training loss = 0.7619929313659668, accuracy = 0.875\n",
      "Training loss = 0.681962788105011, accuracy = 0.8125\n",
      "Training loss = 0.7164828777313232, accuracy = 0.84375\n",
      "Training loss = 0.5278732180595398, accuracy = 0.890625\n",
      "Training loss = 0.6463778018951416, accuracy = 0.859375\n",
      "Training loss = 0.5208379626274109, accuracy = 0.859375\n",
      "Training loss = 0.6154462695121765, accuracy = 0.859375\n",
      "Training loss = 0.5695311427116394, accuracy = 0.828125\n",
      "Training loss = 0.45998188853263855, accuracy = 0.9375\n",
      "Training loss = 0.645228385925293, accuracy = 0.796875\n",
      "Training loss = 0.5100784301757812, accuracy = 0.890625\n",
      "Training loss = 0.42574045062065125, accuracy = 0.90625\n",
      "Training loss = 0.4875563979148865, accuracy = 0.875\n",
      "Training loss = 0.4926314055919647, accuracy = 0.859375\n",
      "Training loss = 0.4929053485393524, accuracy = 0.859375\n",
      "Training loss = 0.6917701959609985, accuracy = 0.828125\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final refactoring\n",
    "- packing dataloaders\n",
    "- packing model and optims\n",
    "- packing fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in, nh, fan_out = 784, 50, 10\n",
    "lr = 0.05\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(fan_in,nh),nn.ReLU(),nn.Linear(nh,fan_out))\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(ds_train, ds_valid, bs=64, **kwargs):\n",
    "    return DataLoader(ds_train, bs, shuffle=True, **kwargs), DataLoader(ds_valid, bs*2, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for ep in range(epochs):\n",
    "        print(f'begin epoch #{ep+1}')\n",
    "        i = 0\n",
    "        # training mode\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            if not i % 200:\n",
    "                acc = accuracy(pred, yb)\n",
    "                print(f'Training loss = {loss.item()}, accuracy = {acc}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            i+=1\n",
    "            \n",
    "        model.eval() # evaluation mode, no gradient calculated\n",
    "        with torch.no_grad():\n",
    "            tot_loss = 0.\n",
    "            tot_acc = 0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred,yb)\n",
    "                tot_acc += accuracy(pred,yb)\n",
    "                \n",
    "        avg_loss = tot_loss/len(valid_dl)\n",
    "        avg_acc = tot_acc/len(valid_dl)\n",
    "        print(f'Average loss = {avg_loss}, validation accuracy = {avg_acc}')\n",
    "    return avg_loss, avg_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset(x_train, y_train) # our dataset class works for pytorch as well\n",
    "ds_valid = Dataset(x_valid, y_valid) # or use pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_dls(ds_train, ds_valid)\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin epoch #1\n",
      "Training loss = 2.35766863822937, accuracy = 0.078125\n",
      "Training loss = 0.6160086393356323, accuracy = 0.859375\n",
      "Training loss = 0.4601333439350128, accuracy = 0.828125\n",
      "Training loss = 0.23151923716068268, accuracy = 0.953125\n",
      "Average loss = 0.320467472076416, validation accuracy = 0.9120846390724182\n",
      "begin epoch #2\n",
      "Training loss = 0.4941417872905731, accuracy = 0.921875\n",
      "Training loss = 0.3273746967315674, accuracy = 0.9375\n",
      "Training loss = 0.3200247883796692, accuracy = 0.890625\n",
      "Training loss = 0.11072078347206116, accuracy = 0.96875\n",
      "Average loss = 0.2671191990375519, validation accuracy = 0.9231606125831604\n",
      "begin epoch #3\n",
      "Training loss = 0.13971638679504395, accuracy = 0.953125\n",
      "Training loss = 0.21798457205295563, accuracy = 0.9375\n",
      "Training loss = 0.16750997304916382, accuracy = 0.984375\n",
      "Training loss = 0.36491021513938904, accuracy = 0.90625\n",
      "Average loss = 0.2291434109210968, validation accuracy = 0.9355221390724182\n",
      "begin epoch #4\n",
      "Training loss = 0.2589975595474243, accuracy = 0.90625\n",
      "Training loss = 0.18829314410686493, accuracy = 0.953125\n",
      "Training loss = 0.35110756754875183, accuracy = 0.90625\n",
      "Training loss = 0.427589476108551, accuracy = 0.90625\n",
      "Average loss = 0.21087771654129028, validation accuracy = 0.9431368708610535\n",
      "begin epoch #5\n",
      "Training loss = 0.2071218192577362, accuracy = 0.90625\n",
      "Training loss = 0.15898996591567993, accuracy = 0.96875\n",
      "Training loss = 0.0918944776058197, accuracy = 0.984375\n",
      "Training loss = 0.18661221861839294, accuracy = 0.921875\n",
      "Average loss = 0.1965111345052719, validation accuracy = 0.9451147317886353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1965), tensor(0.9451))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
